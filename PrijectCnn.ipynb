{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5e815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gil70\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ×”×’×“×¨×ª matplotlib ×œ×¢×‘×¨×™×ª\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "\n",
    "def load_and_prepare_data(data_path_folder):\n",
    "    file_mapping = {\n",
    "        'car_nothing.csv': 'quiet',\n",
    "        'carnew.csv': 'vehicle',\n",
    "        'human_nothing.csv': 'quiet',\n",
    "        'human.csv': 'human'\n",
    "    }\n",
    "    label_encoding = {'quiet': 0, 'vehicle': 1, 'human': 2}\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    print(\"Starting data loading...\")\n",
    "    if not os.path.exists(data_path_folder):\n",
    "        print(f\"Data folder {data_path_folder} not found. Please create it and add data files.\")\n",
    "        return np.array([]), np.array([])\n",
    "    for filename, activity_type in file_mapping.items():\n",
    "        filepath = os.path.join(data_path_folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Error: File not found at {filepath}. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, header=None)\n",
    "            if not df.empty and df.shape[1] > 0:\n",
    "                data = df.iloc[:, 0].values\n",
    "                label_code = label_encoding[activity_type]\n",
    "                all_data.extend(data)\n",
    "                all_labels.extend([label_code] * len(data))\n",
    "            else:\n",
    "                print(f\"Warning: File {filename} is empty or has no data columns. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "    all_data_np = np.array(all_data)\n",
    "    all_labels_np = np.array(all_labels)\n",
    "    if len(all_data_np) > 0:\n",
    "        print(f\"Total data points loaded: {len(all_data_np)}\")\n",
    "    else:\n",
    "        print(\"No data was loaded.\")\n",
    "    return all_data_np, all_labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78aca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(data_dict, labels_dict, window_size_seconds=2, sample_rate=1000):\n",
    "    \"\"\"×”×›× ×ª × ×ª×•× ×™× ×’×•×œ××™×™× ×¢× ×—×œ×•× ×•×ª ×–××Ÿ\"\"\"\n",
    "    \n",
    "    window_size = window_size_seconds * sample_rate  # 2000 ×“×’×™××•×ª\n",
    "    \n",
    "    raw_windows = []\n",
    "    labels = []\n",
    "    \n",
    "    # ×—×™×©×•×‘ ××¡×¤×¨ ×›×•×œ×œ ×©×œ ×—×œ×•× ×•×ª ×œtqdm\n",
    "    total_windows = 0\n",
    "    for filename, signal_data in data_dict.items():\n",
    "        total_windows += (signal_data.shape[0] - window_size) // (window_size // 2)\n",
    "    \n",
    "    print(f\"ğŸ”„ ×™×¦×™×¨×ª {total_windows} ×—×œ×•× ×•×ª ×©×œ {window_size_seconds} ×©× ×™×•×ª...\")\n",
    "    \n",
    "    with tqdm(total=total_windows, desc=\"×¢×™×‘×•×“ ×—×œ×•× ×•×ª ×’×•×œ××™×™×\") as pbar:\n",
    "        for filename, signal_data in data_dict.items():\n",
    "            label = labels_dict[filename]\n",
    "            \n",
    "            # ×—×œ×•× ×•×ª ×¢× overlap ×©×œ 50%\n",
    "            step_size = window_size // 2\n",
    "            \n",
    "            for start_idx in range(0, signal_data.shape[0] - window_size + 1, step_size):\n",
    "                end_idx = start_idx + window_size\n",
    "                window = signal_data[start_idx:end_idx]\n",
    "                \n",
    "                if window.shape[0] == window_size:\n",
    "                    raw_windows.append(window)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                if pbar.n >= total_windows:\n",
    "                    break\n",
    "    \n",
    "    return np.array(raw_windows), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e8be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ××•×“×œ CNN ×¤×©×•×˜ ×œ× ×ª×•× ×™× ×’×•×œ××™×™×\n",
    "def create_raw_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"××•×“×œ CNN ×œ× ×ª×•× ×™× ×’×•×œ××™×™× (2D: ×–××Ÿ Ã— ×—×™×™×©× ×™×)\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # ×”×•×¡×¤×ª dimension channel\n",
    "        layers.Reshape(input_shape + (1,)),\n",
    "        \n",
    "        # CNN layers - ×¨×•××” ×“×¤×•×¡×™× ×‘×–××Ÿ ×•×‘×™×Ÿ ×—×™×™×©× ×™×\n",
    "        layers.Conv2D(16, (5, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(32, (5, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 2), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 2), activation='relu', padding='same'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56621788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_1d_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"××•×“×œ CNN 1D ×©××¢×‘×“ ×›×œ ×—×™×™×©×Ÿ ×‘× ×¤×¨×“ ×•××– ×××—×“\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    input_layer = layers.Input(shape=input_shape)  # (2000, num_sensors)\n",
    "    \n",
    "    # ×¢×™×‘×•×“ ×›×œ ×—×™×™×©×Ÿ ×‘× ×¤×¨×“\n",
    "    sensor_outputs = []\n",
    "    \n",
    "    for i in range(input_shape[1]):  # ×¢×‘×•×¨ ×›×œ ×—×™×™×©×Ÿ\n",
    "        # ×—×™×œ×•×¥ ×—×™×™×©×Ÿ ×‘×•×“×“\n",
    "        sensor_data = layers.Lambda(lambda x, idx=i: x[:, :, idx:idx+1])(input_layer)\n",
    "        \n",
    "        # CNN 1D ×¢×œ ×”×—×™×™×©×Ÿ\n",
    "        x = layers.Conv1D(32, 11, activation='relu', padding='same')(sensor_data)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv1D(64, 7, activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        sensor_outputs.append(x)\n",
    "    \n",
    "    # ××™×—×•×“ ×›×œ ×”×—×™×™×©× ×™×\n",
    "    if len(sensor_outputs) > 1:\n",
    "        combined = layers.Concatenate()(sensor_outputs)\n",
    "    else:\n",
    "        combined = sensor_outputs[0]\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(256, activation='relu')(combined)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa523497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ××™××•×Ÿ ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×\n",
    "def train_raw_model(model_type='2d'):\n",
    "    \"\"\"××™××•×Ÿ ××•×“×œ ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ ×©×œ×‘ 1: ××™××•×Ÿ ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "    data_dict, labels_dict = load_geophone_data()\n",
    "    \n",
    "    if not data_dict:\n",
    "        print(\"âŒ ×œ× × ××¦××• ×§×‘×¦×™ × ×ª×•× ×™×!\")\n",
    "        return None\n",
    "    \n",
    "    # ×”×›× ×ª × ×ª×•× ×™× ×’×•×œ××™×™×\n",
    "    X_raw, y_raw = prepare_raw_data(data_dict, labels_dict, window_size_seconds=2)\n",
    "    \n",
    "    print(f\"âœ… × ×•×¦×¨×• {len(X_raw)} ×—×œ×•× ×•×ª ×’×•×œ××™×™×\")\n",
    "    print(f\"ğŸ“Š ×¦×•×¨×ª ×—×œ×•×Ÿ: {X_raw[0].shape}\")\n",
    "    print(f\"ğŸ“ˆ ×˜×•×•×— ×¢×¨×›×™×: {X_raw.min():.4f} ×¢×“ {X_raw.max():.4f}\")\n",
    "    \n",
    "    # ×§×™×“×•×“ ×ª×•×•×™×•×ª\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_raw)\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded)\n",
    "    \n",
    "    print(f\"ğŸ·ï¸ ×§×˜×’×•×¨×™×•×ª: {label_encoder.classes_}\")\n",
    "    \n",
    "    # × ×•×¨××œ×™×–×¦×™×”\n",
    "    X_normalized = (X_raw - X_raw.mean()) / X_raw.std()\n",
    "    \n",
    "    # ×—×œ×•×§×ª × ×ª×•× ×™×\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_categorical, \n",
    "        test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ××™××•×Ÿ: {X_train.shape[0]} ×“×’×™××•×ª\")\n",
    "    print(f\"ğŸ“‰ ×‘×“×™×§×”: {X_test.shape[0]} ×“×’×™××•×ª\")\n",
    "    \n",
    "    # ×‘× ×™×™×ª ××•×“×œ\n",
    "    print(f\"\\nğŸ—ï¸ ×‘× ×™×™×ª ××•×“×œ CNN {model_type.upper()}...\")\n",
    "    if model_type == '2d':\n",
    "        model = create_raw_cnn_model(X_train[0].shape, len(label_encoder.classes_))\n",
    "    else:\n",
    "        model = create_raw_1d_cnn_model(X_train[0].shape, len(label_encoder.classes_))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‹ ×¡×™×›×•× ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    "    \n",
    "    # ××™××•×Ÿ\n",
    "    print(f\"\\nğŸ¯ ××™××•×Ÿ ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # ×”×¢×¨×›×”\n",
    "    print(\"\\nğŸ“Š ×”×¢×¨×›×ª ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"ğŸ¯ ×“×™×•×§ ××•×“×œ ×’×•×œ××™: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # ×ª×—×–×™×•×ª\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ ×“×•×— ×‘×™×¦×•×¢×™× - ××•×“×œ ×’×•×œ××™:\")\n",
    "    print(classification_report(\n",
    "        y_test_classes, y_pred_classes, \n",
    "        target_names=label_encoder.classes_\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'label_encoder': label_encoder,\n",
    "        'predictions': (y_test_classes, y_pred_classes),\n",
    "        'type': 'raw_' + model_type\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
