{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Pipeline ---\n",
      "--- 1. Project Configuration ---\n",
      "Data Directory: DATATEST\n",
      "Sampling Rate: 1000 Hz\n",
      "Problematic Channels Ignored: {0}\n",
      "Problem Correlation Threshold: 0.8\n",
      "Window Duration: 2 sec (2000 samples)\n",
      "Window Hop Size: 1000 samples (50% overlap)\n",
      "Spectrogram nperseg: 256, noverlap: 192\n",
      "Train/Val/Test Split: Train=~72%, Val=~13%, Test=~15%\n",
      "Batch Size: 32, LR: 1e-05, Epochs: 50, Patience: 4\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# =============================================================================\n",
    "# Full Pipeline: Geophone Data Analysis, Preprocessing, Spectrograms, and CNN Training\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from scipy import signal as sp_signal\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Deep Learning Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Starting Full Pipeline ---\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- 1. תצורה (Configuration) ---\n",
    "# =============================================================================\n",
    "DATA_DIR = 'DATATEST'                  # Directory containing the CSV files\n",
    "SAMPLING_RATE = 1000               # Hz (Sampling rate of the geophones)\n",
    "PROBLEM_CHANNELS = {0}             # Set of problematic channel indices (identified previously)\n",
    "PLOT_SAVE_DIR = \"output_plots\"     # Directory to save analysis plots\n",
    "MODEL_SAVE_PATH = \"best_cnn_model.pth\" # Path to save the best trained model\n",
    "PROBLEM_THRESHOLD = 0.8            # Correlation threshold to flag potential problems between event/nothing (Added)\n",
    "\n",
    "# Windowing Parameters\n",
    "WINDOW_DURATION_SEC = 2           # Length of each window in seconds\n",
    "HOP_RATIO = 0.5                    # Percentage overlap between windows (0.5 = 50%)\n",
    "WINDOW_SIZE_SAMPLES = int(WINDOW_DURATION_SEC * SAMPLING_RATE)\n",
    "HOP_SIZE_SAMPLES = int(WINDOW_SIZE_SAMPLES * HOP_RATIO)\n",
    "\n",
    "# Spectrogram Parameters\n",
    "NPERSEG = 256                      # Length of each segment (FFT window)\n",
    "NOVERLAP_RATIO = 0.75              # Overlap ratio for spectrogram windows\n",
    "NOVERLAP = int(NPERSEG * NOVERLAP_RATIO)\n",
    "WINDOW = 'hann'                    # Window type\n",
    "SCALING = 'density'                # 'density' for PSD, 'spectrum' for STFT magnitude\n",
    "MODE = 'psd'                       # Output mode ('psd', 'magnitude')\n",
    "EPSILON = 1e-10                    # Small value to prevent log(0)\n",
    "\n",
    "# Training Parameters\n",
    "SEED = 42                          # For reproducibility\n",
    "TEST_SIZE = 0.15                   # Proportion for the Test set\n",
    "VALIDATION_SIZE = 0.15             # Proportion for the Validation set (from non-Test data)\n",
    "BATCH_SIZE = 32                    # Number of samples per batch during training\n",
    "LEARNING_RATE = 0.00001             # Initial learning rate for the optimizer\n",
    "NUM_EPOCHS = 50                    # Maximum number of training epochs\n",
    "PATIENCE = 4                      # Early stopping patience (epochs without improvement)\n",
    "SHOW_ANALYSIS_PLOTS = False        # Set to True to show correlation plots interactively (can be many!)\n",
    "\n",
    "print(f\"--- 1. Project Configuration ---\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Sampling Rate: {SAMPLING_RATE} Hz\")\n",
    "print(f\"Problematic Channels Ignored: {PROBLEM_CHANNELS}\")\n",
    "print(f\"Problem Correlation Threshold: {PROBLEM_THRESHOLD}\") # Added printout\n",
    "print(f\"Window Duration: {WINDOW_DURATION_SEC} sec ({WINDOW_SIZE_SAMPLES} samples)\")\n",
    "print(f\"Window Hop Size: {HOP_SIZE_SAMPLES} samples ({HOP_RATIO*100:.0f}% overlap)\")\n",
    "print(f\"Spectrogram nperseg: {NPERSEG}, noverlap: {NOVERLAP}\")\n",
    "print(f\"Train/Val/Test Split: Train=~{1-TEST_SIZE-VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Val=~{VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Test=~{TEST_SIZE:.0%}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}, LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}, Patience: {PATIENCE}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Set Seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Defining Helper Functions ---\n",
      "Helper functions defined.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# --- 2. פונקציות עזר (Helper Functions) ---\n",
    "# =============================================================================\n",
    "print(\"\\n--- 2. Defining Helper Functions ---\")\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    \"\"\"Extracts label (category) from filename.\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    if 'nothing' in filename_lower:\n",
    "        return 'nothing'\n",
    "    elif 'human' in filename_lower or 'man' in filename_lower:\n",
    "        return 'human'\n",
    "    elif 'car' in filename_lower:\n",
    "        return 'car'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_data(data_dir):\n",
    "    \"\"\"Loads CSV data from the directory and organizes it.\"\"\"\n",
    "    data_structured = {}\n",
    "    labels_by_filename = {}\n",
    "    all_source_keys = []\n",
    "    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))\n",
    "    print(f\"\\n--- 2a. Loading Data ---\")\n",
    "    print(f\"Found {len(csv_files)} CSV files in '{data_dir}'.\")\n",
    "    if not csv_files:\n",
    "        print(\"Error: No CSV files found.\")\n",
    "        exit()\n",
    "    for filepath in tqdm(csv_files, desc=\"Loading Files\", unit=\"file\", leave=False):\n",
    "        filename = os.path.basename(filepath)\n",
    "        label = get_label_from_filename(filename)\n",
    "        if label is None:\n",
    "            continue\n",
    "        try:\n",
    "            if os.path.getsize(filepath) == 0:\n",
    "                continue\n",
    "            df = pd.read_csv(filepath, header=None)\n",
    "            if df.empty:\n",
    "                continue\n",
    "            data_structured[filename] = {}\n",
    "            labels_by_filename[filename] = label\n",
    "            for i, col_name in enumerate(df.columns):\n",
    "                if pd.api.types.is_numeric_dtype(df[col_name]):\n",
    "                    sensor_series = df[col_name].fillna(0).values.astype(np.float64)\n",
    "                    # Ensure signal has enough length and variation\n",
    "                    if len(sensor_series) >= 10 and np.std(sensor_series) > 1e-9:\n",
    "                         data_structured[filename][i] = sensor_series\n",
    "                         all_source_keys.append((filename, i))\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    print(f\"\\nFinished loading. Total unique signals (channels) loaded: {len(all_source_keys)}\")\n",
    "    if not all_source_keys:\n",
    "        print(\"Error: No valid signals loaded.\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\nSummary of Loaded Data:\")\n",
    "    for fname, cols_dict in data_structured.items():\n",
    "        print(f\"- File: {fname} (Label: {labels_by_filename[fname]}), Columns: {list(cols_dict.keys())}\")\n",
    "    print(\"-\" * 30)\n",
    "    return data_structured, labels_by_filename, all_source_keys\n",
    "\n",
    "def calculate_cross_correlation(sig1, sig2, sampling_rate):\n",
    "    \"\"\"Calculates normalized cross-correlation and peak info.\"\"\"\n",
    "    sig1 = np.asarray(sig1, dtype=np.float64)\n",
    "    sig2 = np.asarray(sig2, dtype=np.float64)\n",
    "    sig1 = np.nan_to_num(sig1) # Handle potential NaNs\n",
    "    sig2 = np.nan_to_num(sig2)\n",
    "\n",
    "    if len(sig1) < 2 or len(sig2) < 2:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    mean1, std1 = np.mean(sig1), np.std(sig1)\n",
    "    mean2, std2 = np.mean(sig2), np.std(sig2)\n",
    "\n",
    "    # If standard deviation is near zero (constant signal), handle gracefully\n",
    "    if std1 < 1e-9 or std2 < 1e-9 :\n",
    "        n1, n2 = len(sig1), len(sig2)\n",
    "        lags_samples = np.arange(-(n2 - 1), n1)\n",
    "        lags_ms = lags_samples * (1000.0 / sampling_rate)\n",
    "        return lags_ms, np.zeros(len(lags_ms)), 0.0, 0.0, 0 # Return zero correlation\n",
    "\n",
    "    # Normalize signals\n",
    "    sig1_norm = (sig1 - mean1) / std1\n",
    "    sig2_norm = (sig2 - mean2) / std2\n",
    "\n",
    "    # Calculate cross-correlation\n",
    "    correlation = np.correlate(sig1_norm, sig2_norm, mode='full')\n",
    "\n",
    "    # Calculate lags\n",
    "    n1, n2 = len(sig1_norm), len(sig2_norm)\n",
    "    lags_samples = np.arange(-(n2 - 1), n1)\n",
    "    lags_ms = lags_samples * (1000.0 / sampling_rate)\n",
    "\n",
    "    # Normalize the correlation result (optional but often done)\n",
    "    # Note: np.correlate with normalized inputs doesn't guarantee output range [-1, 1]\n",
    "    # A more robust normalization:\n",
    "    # norm_factor = np.sqrt(np.sum(sig1_norm**2) * np.sum(sig2_norm**2)) # Original way\n",
    "    norm_factor = len(sig1_norm) # Alternative normalization factor often used\n",
    "    if norm_factor < 1e-9:\n",
    "        normalized_correlation = np.zeros_like(correlation)\n",
    "    else:\n",
    "        # Ensure normalization doesn't exceed 1 due to potential floating point issues\n",
    "        normalized_correlation = correlation / norm_factor\n",
    "        normalized_correlation = np.clip(normalized_correlation, -1.0, 1.0)\n",
    "\n",
    "\n",
    "    if len(normalized_correlation) == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Find peak\n",
    "    peak_index = np.argmax(np.abs(normalized_correlation))\n",
    "    peak_lag_ms = lags_ms[peak_index]\n",
    "    peak_value = normalized_correlation[peak_index]\n",
    "\n",
    "    # Ensure peak index is valid before calculating sample lag\n",
    "    if peak_index < 0 or peak_index >= len(lags_ms):\n",
    "         peak_lag_ms = None\n",
    "         peak_value = None\n",
    "         peak_lag_samples = 0\n",
    "    else:\n",
    "         peak_lag_samples = int(round(peak_lag_ms * sampling_rate / 1000.0)) if peak_lag_ms is not None else 0\n",
    "\n",
    "\n",
    "    return lags_ms, normalized_correlation, peak_lag_ms, peak_value, peak_lag_samples\n",
    "\n",
    "def plot_cross_correlation(lags_ms, correlation, peak_lag_ms, peak_value, title, filename_to_save=None):\n",
    "    \"\"\"Plots cross-correlation, optionally saves to file.\"\"\"\n",
    "    if lags_ms is None or correlation is None:\n",
    "        return # Cannot plot if data is missing\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.plot(lags_ms, correlation, label='Cross-correlation value', linewidth=1.5)\n",
    "\n",
    "    if peak_lag_ms is not None and peak_value is not None:\n",
    "        ax.scatter([peak_lag_ms], [peak_value], color='red', s=100, zorder=5, label=f'Peak: {peak_value:.2f} at {peak_lag_ms:.1f} ms')\n",
    "    else:\n",
    "        ax.text(0.05, 0.9, \"Peak info unavailable\", transform=ax.transAxes, color='red')\n",
    "\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.7)\n",
    "    ax.axvline(0, color='grey', linestyle='--', linewidth=0.7)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xlabel('Lag (ms)', fontsize=10)\n",
    "    ax.set_ylabel('Norm. Cross-corr [-1, 1]', fontsize=10)\n",
    "    ax.set_ylim([-1.1, 1.1])\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=':')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename_to_save:\n",
    "        try:\n",
    "            plt.savefig(filename_to_save)\n",
    "            # print(f\"  Saved plot: {filename_to_save}\") # Reduce verbosity\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving plot {filename_to_save}: {e}\")\n",
    "        plt.close(fig) # Close after saving\n",
    "    elif SHOW_ANALYSIS_PLOTS:\n",
    "        plt.show() # Show interactively only if requested\n",
    "    else:\n",
    "        plt.close(fig) # Close if not saving and not showing interactively\n",
    "\n",
    "def align_signal(signal_to_align, n_ref, lag_samples):\n",
    "    \"\"\"Aligns a signal based on a given lag relative to a reference length.\"\"\"\n",
    "    n_align = len(signal_to_align)\n",
    "    aligned_signal = np.zeros(n_ref) # Initialize with zeros\n",
    "\n",
    "    if lag_samples > 0: # signal_to_align starts LATER than ref\n",
    "        # Copy from `lag_samples` onwards in signal_to_align into the start of aligned_signal\n",
    "        len_to_copy = min(n_ref, n_align - lag_samples)\n",
    "        if len_to_copy > 0:\n",
    "            aligned_signal[:len_to_copy] = signal_to_align[lag_samples : lag_samples + len_to_copy]\n",
    "    elif lag_samples < 0: # signal_to_align starts EARLIER than ref\n",
    "        # Copy from the start of signal_to_align into aligned_signal starting at `abs(lag_samples)`\n",
    "        start_index_aligned = abs(lag_samples)\n",
    "        len_to_copy = min(n_align, n_ref - start_index_aligned)\n",
    "        if len_to_copy > 0:\n",
    "            aligned_signal[start_index_aligned : start_index_aligned + len_to_copy] = signal_to_align[:len_to_copy]\n",
    "    else: # lag_samples == 0, no shift needed\n",
    "        len_to_copy = min(n_ref, n_align)\n",
    "        aligned_signal[:len_to_copy] = signal_to_align[:len_to_copy]\n",
    "\n",
    "    return aligned_signal\n",
    "\n",
    "print(\"Helper functions defined.\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Correlation Analysis (Identifying Problematic Channels) ---\n",
      "\n",
      "--- 2a. Loading Data ---\n",
      "Found 4 CSV files in 'DATATEST'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished loading. Total unique signals (channels) loaded: 104\n",
      "\n",
      "Summary of Loaded Data:\n",
      "- File: human.csv (Label: human), Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "- File: car.csv (Label: car), Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "- File: car2.csv (Label: car), Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "- File: nothing.csv (Label: nothing), Columns: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "------------------------------\n",
      "Performing 5460 cross-correlation comparisons for analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation analysis complete.\n",
      "\n",
      "No significantly high correlations (>0.8) found between events and 'nothing' on the same channel.\n",
      "\n",
      "Available Channels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "Problematic Channels Ignored: {0}\n",
      "Potentially Good Channels identified for use: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. ניתוח קורלציה וזיהוי עמודות (Correlation Analysis & Channel Selection) ---\n",
    "print(\"\\n--- 3. Correlation Analysis (Identifying Problematic Channels) ---\")\n",
    "data_structured, labels_by_filename, all_source_keys = load_data(DATA_DIR)\n",
    "\n",
    "# Perform all-pairs comparison to document findings\n",
    "all_comparisons = list(itertools.combinations_with_replacement(all_source_keys, 2))\n",
    "print(f\"Performing {len(all_comparisons)} cross-correlation comparisons for analysis...\")\n",
    "results = {}\n",
    "problematic_correlations_summary = {} # Store problematic correlations for summary\n",
    "\n",
    "for key1, key2 in tqdm(all_comparisons, desc=\"Analyzing Correlations\", leave=False):\n",
    "    file1, col1 = key1\n",
    "    file2, col2 = key2\n",
    "\n",
    "    # Ensure data exists for both keys\n",
    "    if file1 not in data_structured or col1 not in data_structured[file1] or \\\n",
    "       file2 not in data_structured or col2 not in data_structured[file2]:\n",
    "       continue # Skip if data is missing\n",
    "\n",
    "    signal1 = data_structured[file1][col1]\n",
    "    signal2 = data_structured[file2][col2]\n",
    "\n",
    "    # Ensure signals have minimum length\n",
    "    if len(signal1) < 2 or len(signal2) < 2:\n",
    "        continue # Skip if signals are too short\n",
    "\n",
    "    lags, corr, peak_lag, peak_val, _ = calculate_cross_correlation(signal1, signal2, SAMPLING_RATE)\n",
    "\n",
    "    if peak_val is not None:\n",
    "        results[(key1, key2)] = peak_val\n",
    "\n",
    "        # בדיקת בעיות ספציפית בין אירוע לשקט באותה עמודה\n",
    "        label1 = labels_by_filename.get(file1)\n",
    "        label2 = labels_by_filename.get(file2)\n",
    "\n",
    "        # Skip self-comparison for problem check\n",
    "        if key1 == key2:\n",
    "            continue\n",
    "\n",
    "        # Check if comparing event vs nothing on the SAME channel\n",
    "        if col1 == col2 and label1 is not None and label2 is not None and \\\n",
    "           ((label1 != 'nothing' and label2 == 'nothing') or (label1 == 'nothing' and label2 != 'nothing')):\n",
    "            if abs(peak_val) > PROBLEM_THRESHOLD:\n",
    "                problematic_correlations_summary[(key1, key2)] = peak_val\n",
    "                PROBLEM_CHANNELS.add(col1) # Update problematic channels set dynamically\n",
    "\n",
    "                # Optional: Plot and save *this specific problematic* correlation for documentation\n",
    "                # safe_title = f\"Problem_Corr_{label1}_vs_{label2}_Col{col1}\".replace(' ','_')\n",
    "                # filename = os.path.join(PLOT_SAVE_DIR, f\"{safe_title}.png\")\n",
    "                # plot_title = f\"High Correlation ({peak_val:.2f})\\n{label1}_Col{col1} vs {label2}_Col{col1}\"\n",
    "                # plot_cross_correlation(lags, corr, peak_lag, peak_val, plot_title, filename_to_save=filename)\n",
    "\n",
    "\n",
    "print(\"\\nCorrelation analysis complete.\")\n",
    "if problematic_correlations_summary:\n",
    "     print(f\"\\n--- Identified Problematic Correlations (Event vs Nothing > {PROBLEM_THRESHOLD}) ---\")\n",
    "     # Sort by absolute correlation value, descending\n",
    "     sorted_problems = sorted(problematic_correlations_summary.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "     for (key1, key2), peak_val in sorted_problems[:10]: # Print top 10 problems\n",
    "         file1, col1 = key1\n",
    "         file2, col2 = key2\n",
    "         label1 = labels_by_filename.get(file1,'?')\n",
    "         label2 = labels_by_filename.get(file2,'?')\n",
    "         print(f\"*   Peak={peak_val:.2f} between [{label1}@{os.path.basename(file1)}_Col{col1}] and [{label2}@{os.path.basename(file2)}_Col{col2}]\")\n",
    "     print(f\"Updated Problematic Channels based on analysis: {PROBLEM_CHANNELS}\")\n",
    "else:\n",
    "     print(f\"\\nNo significantly high correlations (>{PROBLEM_THRESHOLD}) found between events and 'nothing' on the same channel.\")\n",
    "\n",
    "# Determine good channels AFTER analysis\n",
    "all_available_channels = set(col for _, col in all_source_keys)\n",
    "good_channels = sorted(list(all_available_channels - PROBLEM_CHANNELS))\n",
    "print(f\"\\nAvailable Channels: {sorted(list(all_available_channels))}\")\n",
    "print(f\"Problematic Channels Ignored: {PROBLEM_CHANNELS}\")\n",
    "print(f\"Potentially Good Channels identified for use: {good_channels}\")\n",
    "if not good_channels:\n",
    "    print(\"Error: No good channels identified after correlation analysis! Check PROBLEM_THRESHOLD or data.\")\n",
    "    exit()\n",
    "print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Performing Aligned Averaging on Good Channels ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Averaging Files: 100%|██████████| 4/4 [00:08<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished averaging using up to 39 good channels per file (where available). Processed 4 files resulting in averaged signals.\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. מיצוע מיושר (Aligned Averaging on Good Channels) ---\n",
    "print(\"\\n--- 4. Performing Aligned Averaging on Good Channels ---\")\n",
    "averaged_signals = {} # Dictionary: label -> list of averaged signals\n",
    "processed_files_count = 0\n",
    "files_to_process = list(data_structured.keys())\n",
    "\n",
    "for filename in tqdm(files_to_process, desc=\"Averaging Files\"):\n",
    "    if filename not in data_structured:\n",
    "        continue # Should not happen if logic is correct, but safe check\n",
    "\n",
    "    label = labels_by_filename[filename]\n",
    "    available_cols = sorted(list(data_structured[filename].keys()))\n",
    "\n",
    "    # Select only the 'good' channels available in this specific file\n",
    "    good_cols_for_file = [col for col in available_cols if col in good_channels]\n",
    "\n",
    "    if len(good_cols_for_file) < 1:\n",
    "        # tqdm.write(f\"Skipping file {filename} - no good channels available.\")\n",
    "        continue # Skip if no good channels for this file\n",
    "\n",
    "    elif len(good_cols_for_file) == 1:\n",
    "        # If only one good channel, use it directly\n",
    "        averaged_signal = data_structured[filename][good_cols_for_file[0]]\n",
    "    else:\n",
    "        # Multiple good channels: Align and average\n",
    "        # Use the first good channel as the reference\n",
    "        ref_col_idx = good_cols_for_file[0]\n",
    "        signal_ref = data_structured[filename][ref_col_idx]\n",
    "        n_ref = len(signal_ref)\n",
    "\n",
    "        # Start sum with the reference signal\n",
    "        sum_aligned_signals = np.copy(signal_ref).astype(np.float64) # Ensure float for averaging\n",
    "        num_signals_averaged = 1\n",
    "\n",
    "        # Align other good channels to the reference\n",
    "        for i in range(1, len(good_cols_for_file)):\n",
    "            col_to_align_idx = good_cols_for_file[i]\n",
    "            signal_to_align = data_structured[filename][col_to_align_idx]\n",
    "\n",
    "            # Calculate lag relative to reference\n",
    "            _, _, _, _, peak_lag_samples = calculate_cross_correlation(signal_ref, signal_to_align, SAMPLING_RATE)\n",
    "\n",
    "            # If correlation failed or lag is None, skip this channel for averaging\n",
    "            if peak_lag_samples is None:\n",
    "                # tqdm.write(f\"Warning: Could not calculate lag for {filename} Col {col_to_align_idx} vs Col {ref_col_idx}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Align the signal\n",
    "            aligned_signal = align_signal(signal_to_align, n_ref, peak_lag_samples)\n",
    "\n",
    "            # Add to the sum\n",
    "            sum_aligned_signals += aligned_signal\n",
    "            num_signals_averaged += 1\n",
    "\n",
    "        # Calculate the average\n",
    "        if num_signals_averaged > 0:\n",
    "            averaged_signal = sum_aligned_signals / num_signals_averaged\n",
    "        else:\n",
    "            averaged_signal = signal_ref # Fallback to reference if somehow no signals were averaged (shouldn't happen)\n",
    "\n",
    "    # Store the averaged signal for this file under its label\n",
    "    if label not in averaged_signals:\n",
    "        averaged_signals[label] = []\n",
    "    averaged_signals[label].append(averaged_signal)\n",
    "    processed_files_count += 1\n",
    "\n",
    "print(f\"\\nFinished averaging using up to {len(good_channels)} good channels per file (where available). Processed {processed_files_count} files resulting in averaged signals.\")\n",
    "if processed_files_count == 0:\n",
    "    print(\"Error: No files were processed for averaging. Check data or 'good_channels'.\")\n",
    "    exit()\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Windowing Averaged Signals ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished windowing. Total windows created: 102\n",
      "Window counts per label:\n",
      "- car: 44 windows\n",
      "- human: 33 windows\n",
      "- nothing: 25 windows\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 5. חלוקה לחלונות (Windowing Averaged Signals) ---\n",
    "print(\"\\n--- 5. Windowing Averaged Signals ---\")\n",
    "all_windows = []\n",
    "all_window_labels = []\n",
    "labels_to_process = list(averaged_signals.keys())\n",
    "\n",
    "for label in tqdm(labels_to_process, desc=\"Windowing Labels\", leave=False):\n",
    "    if label not in averaged_signals: continue # Safety check\n",
    "    for avg_signal in averaged_signals[label]:\n",
    "        num_samples = len(avg_signal)\n",
    "\n",
    "        # Handle signals shorter than the window size\n",
    "        if num_samples < WINDOW_SIZE_SAMPLES:\n",
    "             # Pad the signal to the required window size\n",
    "             padded_signal = np.pad(avg_signal, (0, WINDOW_SIZE_SAMPLES - num_samples), 'constant', constant_values=0)\n",
    "             all_windows.append(padded_signal)\n",
    "             all_window_labels.append(label)\n",
    "             # tqdm.write(f\"Padded short signal (label: {label}, original len: {num_samples})\")\n",
    "        else:\n",
    "            # Apply sliding window\n",
    "            num_windows = math.floor((num_samples - WINDOW_SIZE_SAMPLES) / HOP_SIZE_SAMPLES) + 1\n",
    "            for i in range(num_windows):\n",
    "                start_idx = i * HOP_SIZE_SAMPLES\n",
    "                end_idx = start_idx + WINDOW_SIZE_SAMPLES\n",
    "                window = avg_signal[start_idx:end_idx]\n",
    "                all_windows.append(window)\n",
    "                all_window_labels.append(label)\n",
    "\n",
    "print(f\"\\nFinished windowing. Total windows created: {len(all_windows)}\")\n",
    "if not all_windows:\n",
    "    print(\"Error: No windows created. Check averaging results or window parameters.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Window counts per label:\")\n",
    "unique_labels, counts = np.unique(all_window_labels, return_counts=True)\n",
    "for l, c in zip(unique_labels, counts):\n",
    "    print(f\"- {l}: {c} windows\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "--- 5b. Analyzing Window Correlations (Optional Analysis Step) ---\n",
      "Analyzing correlations between windows (sampling 100 pairs per comparison type)...\n",
      "Found 44 windows for label 'car'\n",
      "Found 33 windows for label 'human'\n",
      "Found 25 windows for label 'nothing'\n",
      "\n",
      "--- Analyzing Intra-Class Correlations ---\n",
      "Analyzing 100 pairs for label 'car'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 pairs for label 'human'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 pairs for label 'nothing'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Inter-Class Correlations ---\n",
      "Analyzing 100 pairs for 'car' vs 'human'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 pairs for 'car' vs 'nothing'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 100 pairs for 'human' vs 'nothing'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Window Correlation Analysis Summary (Absolute Peak Values) ---\n",
      "* car_vs_car (100 pairs):\n",
      "    Avg: 0.222, Median: 0.202, Std: 0.097, Min: 0.081, Max: 0.767\n",
      "* human_vs_human (100 pairs):\n",
      "    Avg: 0.294, Median: 0.262, Std: 0.113, Min: 0.151, Max: 0.629\n",
      "* nothing_vs_nothing (100 pairs):\n",
      "    Avg: 0.240, Median: 0.209, Std: 0.105, Min: 0.116, Max: 0.680\n",
      "* car_vs_human (100 pairs):\n",
      "    Avg: 0.098, Median: 0.102, Std: 0.031, Min: 0.035, Max: 0.169\n",
      "* car_vs_nothing (100 pairs):\n",
      "    Avg: 0.109, Median: 0.103, Std: 0.031, Min: 0.057, Max: 0.214\n",
      "* human_vs_nothing (100 pairs):\n",
      "    Avg: 0.116, Median: 0.116, Std: 0.027, Min: 0.066, Max: 0.212\n",
      "------------------------------\n",
      "\n",
      "--- 6. Generating Spectrograms from Windows ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# [End of Step 5 code - after the print statements]\n",
    "# ... (code from step 5 finishes here)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- 5b. ניתוח קורלציה בין חלונות (אופציונלי, לניתוח בלבד) ---\n",
    "# =============================================================================\n",
    "print(\"\\n--- 5b. Analyzing Window Correlations (Optional Analysis Step) ---\")\n",
    "\n",
    "# Configuration for window correlation analysis\n",
    "NUM_PAIRS_TO_ANALYZE_PER_COMPARISON = 100 # How many random pairs to check for each comparison type\n",
    "ANALYZE_WINDOW_CORRELATIONS = True       # Set to False to skip this analysis block entirely\n",
    "\n",
    "if ANALYZE_WINDOW_CORRELATIONS and all_windows and all_window_labels:\n",
    "    print(f\"Analyzing correlations between windows (sampling {NUM_PAIRS_TO_ANALYZE_PER_COMPARISON} pairs per comparison type)...\")\n",
    "\n",
    "    # Prepare data: Create a dictionary mapping labels to window indices\n",
    "    indices_by_label = {}\n",
    "    unique_analysis_labels = sorted(list(set(all_window_labels)))\n",
    "    for label in unique_analysis_labels:\n",
    "        indices_by_label[label] = [i for i, lbl in enumerate(all_window_labels) if lbl == label]\n",
    "        print(f\"Found {len(indices_by_label[label])} windows for label '{label}'\")\n",
    "\n",
    "    analysis_results = {} # To store correlation results for different comparisons\n",
    "\n",
    "    # --- 1. Intra-Class Correlation (Within the same label) ---\n",
    "    print(\"\\n--- Analyzing Intra-Class Correlations ---\")\n",
    "    for label in unique_analysis_labels:\n",
    "        label_indices = indices_by_label[label]\n",
    "        if len(label_indices) < 2:\n",
    "            print(f\"Skipping intra-class analysis for '{label}': Not enough windows ({len(label_indices)}).\")\n",
    "            continue\n",
    "\n",
    "        correlations_within_label = []\n",
    "        # Generate unique pairs of indices within this label\n",
    "        possible_pairs = list(itertools.combinations(label_indices, 2))\n",
    "        if len(possible_pairs) == 0: continue\n",
    "\n",
    "        # Sample pairs if there are too many\n",
    "        num_pairs_to_sample = min(NUM_PAIRS_TO_ANALYZE_PER_COMPARISON, len(possible_pairs))\n",
    "        sampled_pairs_indices = random.sample(range(len(possible_pairs)), num_pairs_to_sample)\n",
    "\n",
    "        print(f\"Analyzing {num_pairs_to_sample} pairs for label '{label}'...\")\n",
    "        for pair_idx in tqdm(sampled_pairs_indices, desc=f\"Corr within {label}\", leave=False):\n",
    "             idx1, idx2 = possible_pairs[pair_idx]\n",
    "             window1 = all_windows[idx1]\n",
    "             window2 = all_windows[idx2]\n",
    "\n",
    "             # Use the existing correlation function\n",
    "             _, _, _, peak_val, _ = calculate_cross_correlation(window1, window2, SAMPLING_RATE)\n",
    "\n",
    "             if peak_val is not None:\n",
    "                 correlations_within_label.append(abs(peak_val)) # Store absolute correlation\n",
    "\n",
    "        analysis_results[f\"{label}_vs_{label}\"] = correlations_within_label\n",
    "\n",
    "    # --- 2. Inter-Class Correlation (Between different labels) ---\n",
    "    print(\"\\n--- Analyzing Inter-Class Correlations ---\")\n",
    "    label_combinations = list(itertools.combinations(unique_analysis_labels, 2))\n",
    "\n",
    "    for label1, label2 in label_combinations:\n",
    "        indices1 = indices_by_label[label1]\n",
    "        indices2 = indices_by_label[label2]\n",
    "\n",
    "        if not indices1 or not indices2:\n",
    "            print(f\"Skipping inter-class analysis for '{label1}' vs '{label2}': One or both labels have no windows.\")\n",
    "            continue\n",
    "\n",
    "        correlations_between_labels = []\n",
    "        num_pairs_to_sample = NUM_PAIRS_TO_ANALYZE_PER_COMPARISON\n",
    "\n",
    "        print(f\"Analyzing {num_pairs_to_sample} pairs for '{label1}' vs '{label2}'...\")\n",
    "        # Sample one index from each label list for each pair\n",
    "        sampled_indices1 = random.choices(indices1, k=num_pairs_to_sample)\n",
    "        sampled_indices2 = random.choices(indices2, k=num_pairs_to_sample)\n",
    "\n",
    "        for idx1, idx2 in tqdm(zip(sampled_indices1, sampled_indices2), total=num_pairs_to_sample, desc=f\"Corr {label1} vs {label2}\", leave=False):\n",
    "            window1 = all_windows[idx1]\n",
    "            window2 = all_windows[idx2]\n",
    "\n",
    "            _, _, _, peak_val, _ = calculate_cross_correlation(window1, window2, SAMPLING_RATE)\n",
    "\n",
    "            if peak_val is not None:\n",
    "                correlations_between_labels.append(abs(peak_val)) # Store absolute correlation\n",
    "\n",
    "        analysis_results[f\"{label1}_vs_{label2}\"] = correlations_between_labels\n",
    "\n",
    "    # --- 3. Print Summary Statistics ---\n",
    "    print(\"\\n--- Window Correlation Analysis Summary (Absolute Peak Values) ---\")\n",
    "    for comparison_key, corr_values in analysis_results.items():\n",
    "        if corr_values:\n",
    "            avg_corr = np.mean(corr_values)\n",
    "            max_corr = np.max(corr_values)\n",
    "            min_corr = np.min(corr_values)\n",
    "            std_corr = np.std(corr_values)\n",
    "            median_corr = np.median(corr_values)\n",
    "            count = len(corr_values)\n",
    "            print(f\"* {comparison_key} ({count} pairs):\")\n",
    "            print(f\"    Avg: {avg_corr:.3f}, Median: {median_corr:.3f}, Std: {std_corr:.3f}, Min: {min_corr:.3f}, Max: {max_corr:.3f}\")\n",
    "        else:\n",
    "            print(f\"* {comparison_key}: No correlation values calculated (check data or sampling).\")\n",
    "\n",
    "else:\n",
    "    if not ANALYZE_WINDOW_CORRELATIONS:\n",
    "        print(\"Window correlation analysis is disabled (ANALYZE_WINDOW_CORRELATIONS=False).\")\n",
    "    else:\n",
    "        print(\"Skipping window correlation analysis: No windows or labels available.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "# =============================================================================\n",
    "# --- 6. יצירת ספקטוגרמות (Generating Spectrograms) ---\n",
    "# =============================================================================\n",
    "# [Start of Step 6 code - the print statement and the loop]\n",
    "print(\"\\n--- 6. Generating Spectrograms from Windows ---\")\n",
    "# ... (rest of step 6 code follows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Generating Spectrograms from Windows ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Spectrograms:   0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Spectrograms: 100%|██████████| 102/102 [00:00<00:00, 4193.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 102 spectrograms.\n",
      "------------------------------\n",
      "\n",
      "--- 7. Preparing Data for Training ---\n",
      "Label encoding: [('car', 0), ('human', 1), ('nothing', 2)]\n",
      "Spectrogram shape: (102, 1, 129, 28)\n",
      "Data split: Train=70, Val=16, Test=16\n",
      "Using num_workers=4 for DataLoaders.\n",
      "PyTorch Datasets and DataLoaders created.\n",
      "------------------------------\n",
      "\n",
      "--- 8. Defining CNN Model ---\n",
      "Spectrogram dimensions for model: Height=129, Width=28\n",
      "Model defined and moved to cuda.\n",
      "------------------------------\n",
      "\n",
      "--- 9. Setting up Training ---\n",
      "Loss: CrossEntropyLoss, Optimizer: SGD (LR=1e-05), Scheduler: StepLR\n",
      "------------------------------\n",
      "\n",
      "--- 10. Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [0.2s] - LR: 1.0e-03 - Train Loss: 1.0945, Acc: 42.86% - Val Loss: 1.1024, Acc: 31.25% -> Val Acc Improved (0.00% -> 31.25%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.9064, Acc: 77.14% - Val Loss: 1.0748, Acc: 31.25% -> Val Acc No Improve (1/4). Best: 31.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.7597, Acc: 82.86% - Val Loss: 1.0523, Acc: 43.75% -> Val Acc Improved (31.25% -> 43.75%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.5956, Acc: 87.14% - Val Loss: 1.0204, Acc: 31.25% -> Val Acc No Improve (1/4). Best: 43.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.5117, Acc: 84.29% - Val Loss: 0.9651, Acc: 37.50% -> Val Acc No Improve (2/4). Best: 43.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.4254, Acc: 91.43% - Val Loss: 0.8498, Acc: 50.00% -> Val Acc Improved (43.75% -> 50.00%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.3207, Acc: 98.57% - Val Loss: 0.6549, Acc: 75.00% -> Val Acc Improved (50.00% -> 75.00%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.2640, Acc: 98.57% - Val Loss: 0.4988, Acc: 87.50% -> Val Acc Improved (75.00% -> 87.50%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.2391, Acc: 98.57% - Val Loss: 0.4175, Acc: 87.50% -> Val Acc No Improve (1/4). Best: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.2038, Acc: 97.14% - Val Loss: 0.3691, Acc: 93.75% -> Val Acc Improved (87.50% -> 93.75%). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.1603, Acc: 100.00% - Val Loss: 0.3865, Acc: 87.50% -> Val Acc No Improve (1/4). Best: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.1710, Acc: 100.00% - Val Loss: 0.3192, Acc: 93.75% -> Val Acc No Improve (2/4). Best: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.1380, Acc: 100.00% - Val Loss: 0.2348, Acc: 93.75% -> Val Acc No Improve (3/4). Best: 93.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [0.2s] - LR: 1.0e-03 - Train Loss: 0.0977, Acc: 100.00% - Val Loss: 0.2056, Acc: 93.75% -> Val Acc No Improve (4/4). Best: 93.75%\n",
      "\n",
      "Early stopping triggered after 14 epochs.\n",
      "\n",
      "--- Training Finished (0.0 minutes) ---\n",
      "Best Validation Accuracy achieved: 93.75%\n",
      "Model saved to: best_cnn_model.pth\n",
      "------------------------------\n",
      "\n",
      "--- 11. Evaluating on Test Set ---\n",
      "Loading best model from best_cnn_model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1/1 [00:00<00:00, 11.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 87.50%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       1.00      0.86      0.92         7\n",
      "       human       0.80      0.80      0.80         5\n",
      "     nothing       0.80      1.00      0.89         4\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.87      0.89      0.87        16\n",
      "weighted avg       0.89      0.88      0.88        16\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Confusion matrix saved to output_plots/confusion_matrix_test.png\n",
      "------------------------------\n",
      "\n",
      "--- 12. Plotting Training Curves ---\n",
      "Training history plot saved to output_plots/training_history.png\n",
      "\n",
      "--- Full Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 6. יצירת ספקטוגרמות (Generating Spectrograms) ---\n",
    "print(\"\\n--- 6. Generating Spectrograms from Windows ---\")\n",
    "all_spectrograms = []\n",
    "all_spec_labels = [] # Use a new list in case some spectrograms fail\n",
    "\n",
    "for i, window_signal in enumerate(tqdm(all_windows, desc=\"Generating Spectrograms\")):\n",
    "    try:\n",
    "        frequencies, times, Sxx = sp_signal.spectrogram(\n",
    "            window_signal,\n",
    "            fs=SAMPLING_RATE,\n",
    "            window=WINDOW,\n",
    "            nperseg=NPERSEG,\n",
    "            noverlap=NOVERLAP,\n",
    "            scaling=SCALING,\n",
    "            mode=MODE\n",
    "        )\n",
    "        # Convert to dB, handling potential zeros\n",
    "        log_Sxx = 10 * np.log10(np.maximum(Sxx, EPSILON)) # Use maximum to avoid log(0)\n",
    "        all_spectrograms.append(log_Sxx.astype(np.float32))\n",
    "        all_spec_labels.append(all_window_labels[i]) # Keep label corresponding to successful spectrogram\n",
    "    except ValueError as ve:\n",
    "        # Common error if window size is smaller than nperseg\n",
    "        tqdm.write(f\"Skipping spectrogram for window {i} due to ValueError: {ve}. Check window/spectrogram parameters.\")\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error generating spectrogram for window {i}: {e}\")\n",
    "\n",
    "print(f\"\\nGenerated {len(all_spectrograms)} spectrograms.\")\n",
    "if not all_spectrograms:\n",
    "    print(\"Error: No spectrograms generated successfully.\")\n",
    "    exit()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 7. הכנת נתונים לאימון (Data Preparation for Training) ---\n",
    "print(\"\\n--- 7. Preparing Data for Training ---\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_spec_labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = list(label_encoder.classes_)\n",
    "print(f\"Label encoding: {list(zip(class_names, range(num_classes)))}\")\n",
    "\n",
    "# Add channel dimension for CNN (Channels=1 for grayscale spectrogram)\n",
    "spectrograms_with_channel = np.array(all_spectrograms)[:, np.newaxis, :, :]\n",
    "print(f\"Spectrogram shape: {spectrograms_with_channel.shape}\") # Should be (num_windows, 1, freq_bins, time_bins)\n",
    "\n",
    "# Split data\n",
    "X = spectrograms_with_channel\n",
    "y = encoded_labels\n",
    "\n",
    "# Ensure splitting happens only if there's data\n",
    "if len(X) == 0 or len(y) == 0:\n",
    "    print(\"Error: No data available for splitting.\")\n",
    "    exit()\n",
    "if len(X) != len(y):\n",
    "     print(f\"Error: Mismatch between spectrograms ({len(X)}) and labels ({len(y)}) count.\")\n",
    "     exit()\n",
    "\n",
    "# Stratified split to maintain class proportions\n",
    "# Split into Train+Val and Test\n",
    "try:\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y\n",
    "    )\n",
    "except ValueError as e:\n",
    "     print(f\"Error during Train/Test split: {e}. Not enough samples per class? Total samples: {len(y)}\")\n",
    "     # Fallback: Simple split if stratification fails (though less ideal)\n",
    "     X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "         X, y, test_size=TEST_SIZE, random_state=SEED\n",
    "     )\n",
    "\n",
    "\n",
    "# Split Train+Val into Train and Validation\n",
    "# Adjust validation size relative to the size of the train_val set\n",
    "if len(X_train_val) == 0:\n",
    "     print(\"Error: Train+Val set is empty after first split.\")\n",
    "     exit()\n",
    "val_size_adjusted = VALIDATION_SIZE / (1.0 - TEST_SIZE)\n",
    "if val_size_adjusted >= 1.0:\n",
    "     print(\"Error: Calculated validation size is too large relative to train+val set. Check TEST_SIZE and VALIDATION_SIZE.\")\n",
    "     # Handle edge case: Maybe just use a small fixed number for validation\n",
    "     if len(X_train_val) > 10:\n",
    "          val_size_adjusted = 0.1 # Use 10% of remaining data for validation\n",
    "     else:\n",
    "          print(\"Cannot create validation set with current splits.\")\n",
    "          # Option: Skip validation? Or exit? Let's assign Val = Test for now, but warn.\n",
    "          print(\"Warning: Assigning Test set data to Validation set due to small data size.\")\n",
    "          X_train, X_val, y_train, y_val = X_train_val, X_test, y_train_val, y_test # Not ideal\n",
    "          # exit() # Or exit here\n",
    "\n",
    "else:\n",
    "    try:\n",
    "         X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train_val, y_train_val, test_size=val_size_adjusted, random_state=SEED, stratify=y_train_val\n",
    "         )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during Train/Val split: {e}. Not enough samples per class in train_val set? Size: {len(y_train_val)}\")\n",
    "        # Fallback: Simple split if stratification fails\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=val_size_adjusted, random_state=SEED\n",
    "        )\n",
    "\n",
    "\n",
    "print(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "if len(X_train) == 0:\n",
    "     print(\"Error: Training set is empty.\")\n",
    "     exit()\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.from_numpy(features).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = SpectrogramDataset(X_train, y_train)\n",
    "val_dataset = SpectrogramDataset(X_val, y_val)\n",
    "test_dataset = SpectrogramDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "# Consider num_workers based on your system\n",
    "num_workers = min(os.cpu_count(), 4) if os.cpu_count() else 2 # Heuristic for num_workers\n",
    "print(f\"Using num_workers={num_workers} for DataLoaders.\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "print(\"PyTorch Datasets and DataLoaders created.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 8. הגדרת מודל CNN (CNN Model Definition) ---\n",
    "print(\"\\n--- 8. Defining CNN Model ---\")\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_height, input_width):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1), # (B, 1, H, W) -> (B, 16, H, W)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (B, 16, H/2, W/2)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # (B, 16, H/2, W/2) -> (B, 32, H/2, W/2)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (B, 32, H/4, W/4)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # (B, 32, H/4, W/4) -> (B, 64, H/4, W/4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # (B, 64, H/8, W/8)\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size dynamically based on input dimensions\n",
    "        # We need to know the output size of the conv blocks\n",
    "        # Create a dummy input with the correct size\n",
    "        dummy_input = torch.randn(1, 1, input_height, input_width)\n",
    "        dummy_output = self._forward_conv(dummy_input)\n",
    "        self.flattened_size = int(np.prod(dummy_output.shape)) # Get total number of features\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(), # Flatten the output of conv blocks\n",
    "            nn.Linear(self.flattened_size, 128), # Input size is dynamically calculated\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5), # Regularization\n",
    "            nn.Linear(128, num_classes) # Output layer\n",
    "        )\n",
    "\n",
    "    # Helper function to pass input through conv blocks\n",
    "    def _forward_conv(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_conv(x) # Pass through conv layers\n",
    "        x = self.fc_block(x)      # Pass through fully connected layers\n",
    "        return x\n",
    "\n",
    "# Get spectrogram dimensions from the data\n",
    "if len(spectrograms_with_channel) > 0:\n",
    "    spec_height = spectrograms_with_channel.shape[2]\n",
    "    spec_width = spectrograms_with_channel.shape[3]\n",
    "    print(f\"Spectrogram dimensions for model: Height={spec_height}, Width={spec_width}\")\n",
    "else:\n",
    "    print(\"Error: Cannot determine spectrogram dimensions, no data.\")\n",
    "    # Use placeholder dimensions? Or exit? Let's exit.\n",
    "    exit()\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleCNN(num_classes=num_classes, input_height=spec_height, input_width=spec_width)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model defined and moved to {device}.\")\n",
    "# print(model) # Uncomment to print model structure\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 9. הגדרת אימון (Training Setup) ---\n",
    "print(\"\\n--- 9. Setting up Training ---\")\n",
    "criterion = nn.CrossEntropyLoss() # Suitable for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.859) # SGD with momentum\n",
    "# Learning rate scheduler: reduce LR if training plateaus (e.g., StepLR or ReduceLROnPlateau)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.2) # Reduce LR by factor of 0.2 every 15 epochs\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1) # Alternative: Reduce on validation loss plateau\n",
    "\n",
    "print(f\"Loss: {type(criterion).__name__}, Optimizer: {type(optimizer).__name__} (LR={LEARNING_RATE}), Scheduler: {type(scheduler).__name__}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 10. לולאת אימון ואימות (Training and Validation Loop) ---\n",
    "print(\"\\n--- 10. Starting Training ---\")\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "best_val_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False, unit=\"batch\")\n",
    "    for inputs, labels in train_pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_train_acc = 100.0 * correct_train / total_train\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", leave=False, unit=\"batch\")\n",
    "    with torch.no_grad(): # Disable gradient calculation for validation\n",
    "        for inputs, labels in val_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            val_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset) if len(val_loader.dataset) > 0 else 0\n",
    "    epoch_val_acc = 100.0 * correct_val / total_val if total_val > 0 else 0\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "    # Step the scheduler (adjust LR based on epoch or validation loss)\n",
    "    # If using ReduceLROnPlateau, step with validation loss: scheduler.step(epoch_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr'] # Get current LR for printing\n",
    "    scheduler.step() # For StepLR\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} [{epoch_duration:.1f}s] - LR: {current_lr:.1e} \"\n",
    "          f\"- Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.2f}% \"\n",
    "          f\"- Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.2f}%\", end=\"\") # Print on one line initially\n",
    "\n",
    "    # Check for improvement and Early Stopping\n",
    "    if epoch_val_acc > best_val_accuracy:\n",
    "        print(f\" -> Val Acc Improved ({best_val_accuracy:.2f}% -> {epoch_val_acc:.2f}%). Saving model...\")\n",
    "        best_val_accuracy = epoch_val_acc\n",
    "        try:\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving model: {e}\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\" -> Val Acc No Improve ({epochs_no_improve}/{PATIENCE}). Best: {best_val_accuracy:.2f}%\")\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "            break # Exit training loop\n",
    "\n",
    "training_duration = time.time() - training_start_time\n",
    "print(f\"\\n--- Training Finished ({training_duration / 60:.1f} minutes) ---\")\n",
    "print(f\"Best Validation Accuracy achieved: {best_val_accuracy:.2f}%\")\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 11. הערכה סופית (Final Evaluation on Test Set) ---\n",
    "print(\"\\n--- 11. Evaluating on Test Set ---\")\n",
    "\n",
    "# Check if the best model was saved\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "     print(f\"Error: Best model file not found at {MODEL_SAVE_PATH}. Evaluating with the current model state (last epoch).\")\n",
    "     # If you want to strictly evaluate only the best saved model, uncomment the following:\n",
    "     # print(\"Skipping test evaluation as best model was not saved.\")\n",
    "     # exit()\n",
    "else:\n",
    "    print(f\"Loading best model from {MODEL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    except Exception as e:\n",
    "         print(f\"Error loading saved model state: {e}. Evaluating with current model state.\")\n",
    "\n",
    "model.to(device) # Ensure model is on the correct device\n",
    "model.eval()     # Set model to evaluation mode\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy()) # Move predictions to CPU and store\n",
    "        all_labels.extend(labels.cpu().numpy())   # Move labels to CPU and store\n",
    "\n",
    "# Calculate metrics if predictions were made\n",
    "if all_labels and all_preds:\n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nTest Accuracy: {test_accuracy * 100.0:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    # Use zero_division=0 to handle cases where a class might have no predictions or no true samples in the test set\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix - Test Set')\n",
    "    # Save the plot instead of showing interactively\n",
    "    cm_save_path = os.path.join(PLOT_SAVE_DIR, \"confusion_matrix_test.png\")\n",
    "    try:\n",
    "        plt.savefig(cm_save_path)\n",
    "        print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "        plt.close() # Close the plot after saving\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving confusion matrix plot: {e}\")\n",
    "        plt.show() # Show if saving fails\n",
    "else:\n",
    "    print(\"No predictions or labels found for the test set evaluation.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 12. הצגת גרפי אימון (Plotting Training Curves) ---\n",
    "print(\"\\n--- 12. Plotting Training Curves ---\")\n",
    "\n",
    "# Check if history has data\n",
    "if not history['train_loss'] or not history['val_loss']:\n",
    "    print(\"No training history recorded. Skipping plotting.\")\n",
    "else:\n",
    "    epochs_completed = len(history['train_loss'])\n",
    "    epochs_range = range(1, epochs_completed + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs_range, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, history['train_acc'], 'bo-', label='Training Accuracy')\n",
    "    plt.plot(epochs_range, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.ylim([0, 101]) # Accuracy is between 0 and 100\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.suptitle('Training Metrics')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "\n",
    "    # Save the plot\n",
    "    history_save_path = os.path.join(PLOT_SAVE_DIR, \"training_history.png\")\n",
    "    try:\n",
    "        plt.savefig(history_save_path)\n",
    "        print(f\"Training history plot saved to {history_save_path}\")\n",
    "        plt.close() # Close the plot after saving\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving training history plot: {e}\")\n",
    "        plt.show() # Show if saving fails\n",
    "\n",
    "print(\"\\n--- Full Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
