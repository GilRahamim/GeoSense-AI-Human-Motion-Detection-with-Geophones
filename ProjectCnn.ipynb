{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e815f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ”§ PyTorch device: {device}\")\n",
    "\n",
    "# ×”×’×“×¨×ª matplotlib ×œ×¢×‘×¨×™×ª\n",
    "plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ×¤×•× ×§×¦×™×” ××ª×•×§× ×ª ×œ×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "def load_geophone_data():\n",
    "    \"\"\"×˜×¢×™× ×ª ×›×œ ×§×‘×¦×™ ×”× ×ª×•× ×™×\"\"\"\n",
    "    \n",
    "    file_mapping = {\n",
    "        'man.csv': 'human',\n",
    "        'car.csv': 'vehicle', \n",
    "        'car2.csv': 'vehicle',\n",
    "        'nothing.csv': 'quiet'\n",
    "    }\n",
    "    \n",
    "    data_dict = {}\n",
    "    labels_dict = {}\n",
    "    \n",
    "    print(\"ğŸ”„ ×˜×¢×™× ×ª ×§×‘×¦×™ × ×ª×•× ×™×...\")\n",
    "    \n",
    "    for filename, label in file_mapping.items():\n",
    "        try:\n",
    "            # ×§×¨×™××ª ×”×§×•×‘×¥\n",
    "            df = pd.read_csv(filename, header=None)\n",
    "            print(f\"âœ… × ×˜×¢×Ÿ {filename}: {df.shape[0]} ×©×•×¨×•×ª, {df.shape[1]} ×¢××•×“×•×ª\")\n",
    "            \n",
    "            # ×”××¨×” ×œ××¢×¨×š numpy\n",
    "            data_dict[filename] = df.values\n",
    "            labels_dict[filename] = label\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"âŒ ×œ× × ××¦× ×§×•×‘×¥: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ×©×’×™××” ×‘×˜×¢×™× ×ª {filename}: {e}\")\n",
    "    \n",
    "    return data_dict, labels_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(data_dict, labels_dict, window_size_seconds=2, sample_rate=1000):\n",
    "    \"\"\"×”×›× ×ª × ×ª×•× ×™× ×’×•×œ××™×™× ×¢× ×—×œ×•× ×•×ª ×–××Ÿ\"\"\"\n",
    "    \n",
    "    window_size = window_size_seconds * sample_rate  # 2000 ×“×’×™××•×ª\n",
    "    \n",
    "    raw_windows = []\n",
    "    labels = []\n",
    "    \n",
    "    # ×—×™×©×•×‘ ××¡×¤×¨ ×›×•×œ×œ ×©×œ ×—×œ×•× ×•×ª ×œtqdm\n",
    "    total_windows = 0\n",
    "    for filename, signal_data in data_dict.items():\n",
    "        total_windows += (signal_data.shape[0] - window_size) // (window_size // 2)\n",
    "    \n",
    "    print(f\"ğŸ”„ ×™×¦×™×¨×ª {total_windows} ×—×œ×•× ×•×ª ×©×œ {window_size_seconds} ×©× ×™×•×ª...\")\n",
    "    \n",
    "    with tqdm(total=total_windows, desc=\"×¢×™×‘×•×“ ×—×œ×•× ×•×ª ×’×•×œ××™×™×\") as pbar:\n",
    "        for filename, signal_data in data_dict.items():\n",
    "            label = labels_dict[filename]\n",
    "            \n",
    "            # ×—×œ×•× ×•×ª ×¢× overlap ×©×œ 50%\n",
    "            step_size = window_size // 2\n",
    "            \n",
    "            for start_idx in range(0, signal_data.shape[0] - window_size + 1, step_size):\n",
    "                end_idx = start_idx + window_size\n",
    "                window = signal_data[start_idx:end_idx]\n",
    "                \n",
    "                if window.shape[0] == window_size:\n",
    "                    raw_windows.append(window)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                if pbar.n >= total_windows:\n",
    "                    break\n",
    "    \n",
    "    return np.array(raw_windows), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd8f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"××•×“×œ CNN ×œ× ×ª×•× ×™× ×’×•×œ××™×™× (2D: ×–××Ÿ Ã— ×—×™×™×©× ×™×)\"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # ×”×•×¡×¤×ª dimension channel\n",
    "        layers.Reshape(input_shape + (1,)),\n",
    "        \n",
    "        # CNN layers - ×¨×•××” ×“×¤×•×¡×™× ×‘×–××Ÿ ×•×‘×™×Ÿ ×—×™×™×©× ×™×\n",
    "        layers.Conv2D(16, (5, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(32, (5, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 2), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 1)),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 2), activation='relu', padding='same'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_1d_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"××•×“×œ CNN 1D ×©××¢×‘×“ ×›×œ ×—×™×™×©×Ÿ ×‘× ×¤×¨×“ ×•××– ×××—×“\"\"\"\n",
    "    \n",
    "    # Input\n",
    "    input_layer = layers.Input(shape=input_shape)  # (2000, num_sensors)\n",
    "    \n",
    "    # ×¢×™×‘×•×“ ×›×œ ×—×™×™×©×Ÿ ×‘× ×¤×¨×“\n",
    "    sensor_outputs = []\n",
    "    \n",
    "    for i in range(input_shape[1]):  # ×¢×‘×•×¨ ×›×œ ×—×™×™×©×Ÿ\n",
    "        # ×—×™×œ×•×¥ ×—×™×™×©×Ÿ ×‘×•×“×“\n",
    "        sensor_data = layers.Lambda(lambda x, idx=i: x[:, :, idx:idx+1])(input_layer)\n",
    "        \n",
    "        # CNN 1D ×¢×œ ×”×—×™×™×©×Ÿ\n",
    "        x = layers.Conv1D(32, 11, activation='relu', padding='same')(sensor_data)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv1D(64, 7, activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = layers.GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        sensor_outputs.append(x)\n",
    "    \n",
    "    # ××™×—×•×“ ×›×œ ×”×—×™×™×©× ×™×\n",
    "    if len(sensor_outputs) > 1:\n",
    "        combined = layers.Concatenate()(sensor_outputs)\n",
    "    else:\n",
    "        combined = sensor_outputs[0]\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(256, activation='relu')(combined)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output\n",
    "    output = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raw_model(model_type='2d'):\n",
    "    \"\"\"××™××•×Ÿ ××•×“×œ ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ ×©×œ×‘ 1: ××™××•×Ÿ ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "    data_dict, labels_dict = load_geophone_data()\n",
    "    \n",
    "    if not data_dict:\n",
    "        print(\"âŒ ×œ× × ××¦××• ×§×‘×¦×™ × ×ª×•× ×™×!\")\n",
    "        return None\n",
    "    \n",
    "    # ×”×›× ×ª × ×ª×•× ×™× ×’×•×œ××™×™×\n",
    "    X_raw, y_raw = prepare_raw_data(data_dict, labels_dict, window_size_seconds=2)\n",
    "    \n",
    "    print(f\"âœ… × ×•×¦×¨×• {len(X_raw)} ×—×œ×•× ×•×ª ×’×•×œ××™×™×\")\n",
    "    print(f\"ğŸ“Š ×¦×•×¨×ª ×—×œ×•×Ÿ: {X_raw[0].shape}\")\n",
    "    print(f\"ğŸ“ˆ ×˜×•×•×— ×¢×¨×›×™×: {X_raw.min():.4f} ×¢×“ {X_raw.max():.4f}\")\n",
    "    \n",
    "    # ×§×™×“×•×“ ×ª×•×•×™×•×ª\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_raw)\n",
    "    y_categorical = tf.keras.utils.to_categorical(y_encoded)\n",
    "    \n",
    "    print(f\"ğŸ·ï¸ ×§×˜×’×•×¨×™×•×ª: {label_encoder.classes_}\")\n",
    "    \n",
    "    # × ×•×¨××œ×™×–×¦×™×”\n",
    "    X_normalized = (X_raw - X_raw.mean()) / X_raw.std()\n",
    "    \n",
    "    # ×—×œ×•×§×ª × ×ª×•× ×™×\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_normalized, y_categorical, \n",
    "        test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“ˆ ××™××•×Ÿ: {X_train.shape[0]} ×“×’×™××•×ª\")\n",
    "    print(f\"ğŸ“‰ ×‘×“×™×§×”: {X_test.shape[0]} ×“×’×™××•×ª\")\n",
    "    \n",
    "    # ×‘× ×™×™×ª ××•×“×œ\n",
    "    print(f\"\\nğŸ—ï¸ ×‘× ×™×™×ª ××•×“×œ CNN {model_type.upper()}...\")\n",
    "    if model_type == '2d':\n",
    "        model = create_raw_cnn_model(X_train[0].shape, len(label_encoder.classes_))\n",
    "    else:\n",
    "        model = create_raw_1d_cnn_model(X_train[0].shape, len(label_encoder.classes_))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‹ ×¡×™×›×•× ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    "    \n",
    "    # ××™××•×Ÿ\n",
    "    print(f\"\\nğŸ¯ ××™××•×Ÿ ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # ×”×¢×¨×›×”\n",
    "    print(\"\\nğŸ“Š ×”×¢×¨×›×ª ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"ğŸ¯ ×“×™×•×§ ××•×“×œ ×’×•×œ××™: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # ×ª×—×–×™×•×ª\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ ×“×•×— ×‘×™×¦×•×¢×™× - ××•×“×œ ×’×•×œ××™:\")\n",
    "    print(classification_report(\n",
    "        y_test_classes, y_pred_classes, \n",
    "        target_names=label_encoder.classes_\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'label_encoder': label_encoder,\n",
    "        'predictions': (y_test_classes, y_pred_classes),\n",
    "        'type': 'raw_' + model_type\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecf2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeophoneFeatureExtractor:\n",
    "    def __init__(self, sample_rate=1000, window_size=2048, hop_length=512):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_size\n",
    "        self.hop_length = hop_length\n",
    "    \n",
    "    def extract_mel_spectrogram(self, signal_data, channel=0):\n",
    "        \"\"\"×—×™×œ×•×¥ ××œ-×¡×¤×§×˜×•×’×¨××”\"\"\"\n",
    "        if channel >= signal_data.shape[1]:\n",
    "            channel = 0\n",
    "            \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=signal_data[:, channel].astype(float),\n",
    "            sr=self.sample_rate,\n",
    "            n_mels=128,\n",
    "            hop_length=self.hop_length,\n",
    "            n_fft=self.window_size\n",
    "        )\n",
    "        \n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "    \n",
    "    def extract_fft_features(self, signal_data):\n",
    "        \"\"\"×—×™×œ×•×¥ ×××¤×™×™× ×™× ×‘×××¦×¢×•×ª FFT\"\"\"\n",
    "        fft_features = []\n",
    "        \n",
    "        for i in range(signal_data.shape[1]):\n",
    "            fft_vals = fft(signal_data[:, i])\n",
    "            fft_magnitude = np.abs(fft_vals[:len(fft_vals)//2])\n",
    "            \n",
    "            features = [\n",
    "                np.mean(fft_magnitude),\n",
    "                np.std(fft_magnitude),\n",
    "                np.max(fft_magnitude),\n",
    "                np.sum(fft_magnitude),\n",
    "                np.median(fft_magnitude),\n",
    "                np.percentile(fft_magnitude, 75),\n",
    "                np.percentile(fft_magnitude, 25)\n",
    "            ]\n",
    "            fft_features.extend(features)\n",
    "        \n",
    "        return np.array(fft_features)\n",
    "    \n",
    "    def extract_time_domain_features(self, signal_data):\n",
    "        \"\"\"×—×™×œ×•×¥ ×××¤×™×™× ×™× ×‘×ª×—×•× ×”×–××Ÿ\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for i in range(signal_data.shape[1]):\n",
    "            channel_data = signal_data[:, i]\n",
    "            \n",
    "            features.extend([\n",
    "                np.mean(channel_data),\n",
    "                np.std(channel_data),\n",
    "                np.var(channel_data),\n",
    "                np.max(channel_data),\n",
    "                np.min(channel_data),\n",
    "                np.ptp(channel_data),\n",
    "                np.mean(np.abs(channel_data)),\n",
    "                np.sqrt(np.mean(channel_data**2)),\n",
    "            ])\n",
    "            \n",
    "            # Zero crossing rate\n",
    "            zero_crossings = np.where(np.diff(np.signbit(channel_data)))[0]\n",
    "            zcr = len(zero_crossings) / len(channel_data)\n",
    "            features.append(zcr)\n",
    "            \n",
    "            # Spectral centroid\n",
    "            try:\n",
    "                spectral_centroid = librosa.feature.spectral_centroid(\n",
    "                    y=channel_data.astype(float), sr=self.sample_rate\n",
    "                )[0]\n",
    "                features.append(np.mean(spectral_centroid))\n",
    "            except:\n",
    "                features.append(0)\n",
    "        \n",
    "        return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_advanced_data(data_dict, labels_dict, segment_length=2000):\n",
    "    \"\"\"×”×›× ×ª × ×ª×•× ×™× ×¢× ×××¤×™×™× ×™× ××ª×§×“××™×\"\"\"\n",
    "    \n",
    "    extractor = GeophoneFeatureExtractor()\n",
    "    \n",
    "    spectrograms = []\n",
    "    fft_features = []\n",
    "    time_features = []\n",
    "    labels = []\n",
    "    \n",
    "    total_segments = sum(signal_data.shape[0] // segment_length \n",
    "                        for signal_data in data_dict.values())\n",
    "    \n",
    "    print(f\"ğŸ”„ ×¢×™×‘×•×“ {total_segments} ×¡×’×× ×˜×™× ××ª×§×“××™×...\")\n",
    "    \n",
    "    with tqdm(total=total_segments, desc=\"×¢×™×‘×•×“ ×××¤×™×™× ×™× ××ª×§×“××™×\") as pbar:\n",
    "        for filename, signal_data in data_dict.items():\n",
    "            label = labels_dict[filename]\n",
    "            num_segments = signal_data.shape[0] // segment_length\n",
    "            \n",
    "            for i in range(num_segments):\n",
    "                start_idx = i * segment_length\n",
    "                end_idx = start_idx + segment_length\n",
    "                segment = signal_data[start_idx:end_idx]\n",
    "                \n",
    "                if segment.shape[0] < segment_length:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    mel_spec = extractor.extract_mel_spectrogram(segment, channel=0)\n",
    "                    fft_feat = extractor.extract_fft_features(segment)\n",
    "                    time_feat = extractor.extract_time_domain_features(segment)\n",
    "                    \n",
    "                    spectrograms.append(mel_spec)\n",
    "                    fft_features.append(fft_feat)\n",
    "                    time_features.append(time_feat)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"×©×’×™××” ×‘×¢×™×‘×•×“: {e}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    return np.array(spectrograms), np.array(fft_features), np.array(time_features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1525529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_model(spectrogram_shape, fft_features_shape, time_features_shape, num_classes):\n",
    "    \"\"\"××•×“×œ ××ª×§×“× ×¢× ×¡×¤×§×˜×•×’×¨××” + ×××¤×™×™× ×™×\"\"\"\n",
    "    \n",
    "    # Inputs\n",
    "    spectrogram_input = layers.Input(shape=spectrogram_shape, name='spectrogram_input')\n",
    "    fft_input = layers.Input(shape=(fft_features_shape,), name='fft_input')\n",
    "    time_input = layers.Input(shape=(time_features_shape,), name='time_input')\n",
    "    \n",
    "    # CNN ×œ×¡×¤×§×˜×•×’×¨××”\n",
    "    x1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(spectrogram_input)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x1 = layers.MaxPooling2D((2, 2))(x1)\n",
    "    x1 = layers.BatchNormalization()(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x1 = layers.GlobalAveragePooling2D()(x1)\n",
    "    x1 = layers.Dense(256, activation='relu')(x1)\n",
    "    x1 = layers.Dropout(0.3)(x1)\n",
    "    \n",
    "    # Dense ×œ×××¤×™×™× ×™ FFT\n",
    "    x2 = layers.Dense(128, activation='relu')(fft_input)\n",
    "    x2 = layers.BatchNormalization()(x2)\n",
    "    x2 = layers.Dropout(0.3)(x2)\n",
    "    x2 = layers.Dense(64, activation='relu')(x2)\n",
    "    \n",
    "    # Dense ×œ×××¤×™×™× ×™× ×‘×–××Ÿ\n",
    "    x3 = layers.Dense(128, activation='relu')(time_input)\n",
    "    x3 = layers.BatchNormalization()(x3)\n",
    "    x3 = layers.Dropout(0.3)(x3)\n",
    "    x3 = layers.Dense(64, activation='relu')(x3)\n",
    "    \n",
    "    # ××™×—×•×“\n",
    "    combined = layers.Concatenate()([x1, x2, x3])\n",
    "    combined = layers.Dense(512, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.5)(combined)\n",
    "    combined = layers.Dense(256, activation='relu')(combined)\n",
    "    combined = layers.Dropout(0.3)(combined)\n",
    "    \n",
    "    output = layers.Dense(num_classes, activation='softmax')(combined)\n",
    "    \n",
    "    model = keras.Model(inputs=[spectrogram_input, fft_input, time_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9257ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_model():\n",
    "    \"\"\"××™××•×Ÿ ××•×“×œ ××ª×§×“×\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ”„ ×©×œ×‘ 2: ××™××•×Ÿ ××•×“×œ ××ª×§×“×\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "    data_dict, labels_dict = load_geophone_data()\n",
    "    \n",
    "    # ×”×›× ×ª ×××¤×™×™× ×™× ××ª×§×“××™×\n",
    "    spectrograms, fft_features, time_features, labels = prepare_advanced_data(data_dict, labels_dict)\n",
    "    \n",
    "    print(f\"âœ… × ×•×¦×¨×• {len(spectrograms)} ×¡×¤×§×˜×•×’×¨××•×ª ××ª×§×“××•×ª\")\n",
    "    print(f\"ğŸ“Š ×¦×•×¨×ª ×¡×¤×§×˜×•×’×¨××”: {spectrograms[0].shape}\")\n",
    "    \n",
    "    # ×§×™×“×•×“ ×ª×•×•×™×•×ª\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    labels_categorical = tf.keras.utils.to_categorical(labels_encoded)\n",
    "    \n",
    "    # × ×•×¨××œ×™×–×¦×™×”\n",
    "    scaler_fft = StandardScaler()\n",
    "    scaler_time = StandardScaler()\n",
    "    \n",
    "    fft_features_normalized = scaler_fft.fit_transform(fft_features)\n",
    "    time_features_normalized = scaler_time.fit_transform(time_features)\n",
    "    \n",
    "    # ×—×œ×•×§×ª × ×ª×•× ×™×\n",
    "    (X_spec_train, X_spec_test, \n",
    "     X_fft_train, X_fft_test, \n",
    "     X_time_train, X_time_test, \n",
    "     y_train, y_test) = train_test_split(\n",
    "        spectrograms, fft_features_normalized, time_features_normalized, labels_categorical,\n",
    "        test_size=0.2, random_state=42, stratify=labels_encoded\n",
    "    )\n",
    "    \n",
    "    # × ×•×¨××œ×™×–×¦×™×” ×©×œ ×¡×¤×§×˜×•×’×¨××•×ª\n",
    "    X_spec_train = X_spec_train / 255.0\n",
    "    X_spec_test = X_spec_test / 255.0\n",
    "    \n",
    "    if len(X_spec_train.shape) == 3:\n",
    "        X_spec_train = np.expand_dims(X_spec_train, -1)\n",
    "        X_spec_test = np.expand_dims(X_spec_test, -1)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ × ×ª×•× ×™ ××™××•×Ÿ ××ª×§×“××™×: ×¡×¤×§×˜×•×’×¨××•×ª {X_spec_train.shape}\")\n",
    "    \n",
    "    # ×‘× ×™×™×ª ××•×“×œ\n",
    "    print(\"\\nğŸ—ï¸ ×‘× ×™×™×ª ××•×“×œ ××ª×§×“×...\")\n",
    "    model = create_advanced_model(\n",
    "        X_spec_train[0].shape, \n",
    "        X_fft_train.shape[1], \n",
    "        X_time_train.shape[1], \n",
    "        len(label_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“‹ ×¡×™×›×•× ××•×“×œ ××ª×§×“×:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # ××™××•×Ÿ\n",
    "    print(\"\\nğŸ¯ ××™××•×Ÿ ××•×“×œ ××ª×§×“×...\")\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=7)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        [X_spec_train, X_fft_train, X_time_train], y_train,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # ×”×¢×¨×›×”\n",
    "    print(\"\\nğŸ“Š ×”×¢×¨×›×ª ××•×“×œ ××ª×§×“×...\")\n",
    "    test_loss, test_accuracy = model.evaluate(\n",
    "        [X_spec_test, X_fft_test, X_time_test], y_test, verbose=0\n",
    "    )\n",
    "    print(f\"ğŸ¯ ×“×™×•×§ ××•×“×œ ××ª×§×“×: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # ×ª×—×–×™×•×ª\n",
    "    y_pred = model.predict([X_spec_test, X_fft_test, X_time_test], verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ ×“×•×— ×‘×™×¦×•×¢×™× - ××•×“×œ ××ª×§×“×:\")\n",
    "    print(classification_report(\n",
    "        y_test_classes, y_pred_classes, \n",
    "        target_names=label_encoder.classes_\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'label_encoder': label_encoder,\n",
    "        'predictions': (y_test_classes, y_pred_classes),\n",
    "        'type': 'advanced'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(results_list):\n",
    "    \"\"\"×”×©×•×•××ª ×ª×•×¦××•×ª ×”××•×“×œ×™× ×”×©×•× ×™×\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ† ×”×©×•×•××ª ×ª×•×¦××•×ª\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ×™×¦×™×¨×ª ×˜×‘×œ×ª ×”×©×•×•××”\n",
    "    comparison_data = []\n",
    "    for result in results_list:\n",
    "        if result:\n",
    "            comparison_data.append({\n",
    "                '××•×“×œ': result['type'],\n",
    "                '×“×™×•×§': f\"{result['test_accuracy']:.4f}\",\n",
    "                '×“×™×•×§ %': f\"{result['test_accuracy']*100:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # ×’×¨×£ ×”×©×•×•××”\n",
    "    if len(comparison_data) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        models = [item['××•×“×œ'] for item in comparison_data]\n",
    "        accuracies = [float(item['×“×™×•×§']) for item in comparison_data]\n",
    "        \n",
    "        bars = plt.bar(models, accuracies, color=['skyblue', 'lightgreen', 'orange'][:len(models)])\n",
    "        plt.title('×”×©×•×•××ª ×“×™×•×§ ×”××•×“×œ×™× ×”×©×•× ×™×', fontsize=16, fontweight='bold')\n",
    "        plt.ylabel('×“×™×•×§')\n",
    "        plt.xlabel('×¡×•×’ ××•×“×œ')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # ×”×•×¡×¤×ª ×¢×¨×›×™× ×¢×œ ×”×¢××•×“×•×ª\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # ×”××œ×¦×•×ª\n",
    "    print(\"\\nğŸ’¡ ××¡×§× ×•×ª ×•×”××œ×¦×•×ª:\")\n",
    "    if len(comparison_data) > 1:\n",
    "        best_model = max(comparison_data, key=lambda x: float(x['×“×™×•×§']))\n",
    "        print(f\"ğŸ¥‡ ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨: {best_model['××•×“×œ']} ×¢× ×“×™×•×§ ×©×œ {best_model['×“×™×•×§ %']}\")\n",
    "        \n",
    "        worst_model = min(comparison_data, key=lambda x: float(x['×“×™×•×§']))\n",
    "        improvement = (float(best_model['×“×™×•×§']) - float(worst_model['×“×™×•×§'])) * 100\n",
    "        print(f\"ğŸ“ˆ ×©×™×¤×•×¨ ×©×œ {improvement:.2f}% ×‘×™×Ÿ ×”××•×“×œ ×”×’×¨×•×¢ ×‘×™×•×ª×¨ ×œ×˜×•×‘ ×‘×™×•×ª×¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_comparison():\n",
    "    \"\"\"×”×¨×¦×ª ×”×©×•×•××” ××œ××” ×‘×™×Ÿ ×”×’×™×©×•×ª\"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ ×”×ª×—×œ×ª ×”×©×•×•××” ××§×™×¤×” - × ×ª×•× ×™× ×’×•×œ××™×™× vs ××ª×§×“×\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # ×©×œ×‘ 1: ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™× 2D\n",
    "    print(\"\\n1ï¸âƒ£ ××™××•×Ÿ ××•×“×œ CNN 2D ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    try:\n",
    "        result_raw_2d = train_raw_model(model_type='2d')\n",
    "        results.append(result_raw_2d)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘××•×“×œ ×’×•×œ××™ 2D: {e}\")\n",
    "    \n",
    "    # ×©×œ×‘ 2: ××•×“×œ × ×ª×•× ×™× ×’×•×œ××™×™× 1D\n",
    "    print(\"\\n2ï¸âƒ£ ××™××•×Ÿ ××•×“×œ CNN 1D ×¢×œ × ×ª×•× ×™× ×’×•×œ××™×™×...\")\n",
    "    try:\n",
    "        result_raw_1d = train_raw_model(model_type='1d')\n",
    "        results.append(result_raw_1d)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘××•×“×œ ×’×•×œ××™ 1D: {e}\")\n",
    "    \n",
    "    # ×©×œ×‘ 3: ××•×“×œ ××ª×§×“×\n",
    "    print(\"\\n3ï¸âƒ£ ××™××•×Ÿ ××•×“×œ ××ª×§×“×...\")\n",
    "    try:\n",
    "        result_advanced = train_advanced_model()\n",
    "        results.append(result_advanced)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘××•×“×œ ××ª×§×“×: {e}\")\n",
    "    \n",
    "    # ×”×©×•×•××ª ×ª×•×¦××•×ª\n",
    "    print(\"\\nğŸ” ×”×©×•×•××ª ×›×œ ×”×ª×•×¦××•×ª...\")\n",
    "    compare_results(results)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a8c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison_detailed(results_list):\n",
    "    \"\"\"×”×¦×’×ª ×”×©×•×•××” ××¤×•×¨×˜×ª ×©×œ ×”××•×“×œ×™×\"\"\"\n",
    "    \n",
    "    if len(results_list) < 2:\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('×”×©×•×•××” ××¤×•×¨×˜×ª ×‘×™×Ÿ ×”××•×“×œ×™×', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # ×’×¨×£ 1: ×“×™×•×§\n",
    "    axes[0, 0].bar([r['type'] for r in results_list if r], \n",
    "                   [r['test_accuracy'] for r in results_list if r],\n",
    "                   color=['lightblue', 'lightgreen', 'orange'])\n",
    "    axes[0, 0].set_title('×“×™×•×§ ×”××•×“×œ×™×')\n",
    "    axes[0, 0].set_ylabel('×“×™×•×§')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # ×’×¨×£ 2: ×”×™×¡×˜×•×¨×™×™×ª ××™××•×Ÿ (×× ×™×©)\n",
    "    for i, result in enumerate(results_list):\n",
    "        if result and 'history' in result:\n",
    "            history = result['history']\n",
    "            if 'accuracy' in history.history:\n",
    "                axes[0, 1].plot(history.history['accuracy'], \n",
    "                              label=f\"{result['type']} - ××™××•×Ÿ\", linewidth=2)\n",
    "                if 'val_accuracy' in history.history:\n",
    "                    axes[0, 1].plot(history.history['val_accuracy'], \n",
    "                                  label=f\"{result['type']} - ×•×œ×™×“×¦×™×”\", \n",
    "                                  linestyle='--', linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_title('×”×ª×§×“××•×ª ×”×“×™×•×§ ×‘××”×œ×š ×”××™××•×Ÿ')\n",
    "    axes[0, 1].set_xlabel('××¤×•×§')\n",
    "    axes[0, 1].set_ylabel('×“×™×•×§')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ×’×¨×£ 3: ××˜×¨×™×¦×•×ª ×‘×œ×‘×•×œ\n",
    "    for i, result in enumerate(results_list[:2]):  # ×¨×§ 2 ×¨××©×•× ×™×\n",
    "        if result and 'predictions' in result:\n",
    "            y_true, y_pred = result['predictions']\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            \n",
    "            im = axes[1, i].imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "            axes[1, i].set_title(f'××˜×¨×™×¦×ª ×‘×œ×‘×•×œ - {result[\"type\"]}')\n",
    "            \n",
    "            # ×”×•×¡×¤×ª ××¡×¤×¨×™×\n",
    "            thresh = cm.max() / 2.\n",
    "            for j in range(cm.shape[0]):\n",
    "                for k in range(cm.shape[1]):\n",
    "                    axes[1, i].text(k, j, format(cm[j, k], 'd'),\n",
    "                                  ha=\"center\", va=\"center\",\n",
    "                                  color=\"white\" if cm[j, k] > thresh else \"black\")\n",
    "    \n",
    "    # ×”×¡×ª×¨×ª ×’×¨×£ ×¨×‘×™×¢×™ ×× ××™×Ÿ ×¦×•×¨×š\n",
    "    if len(results_list) < 3:\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_raw_data_samples():\n",
    "    \"\"\"×”×¦×’×ª ×“×•×’×××•×ª ××”× ×ª×•× ×™× ×”×’×•×œ××™×™×\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š ×”×¦×’×ª ×“×•×’×××•×ª ××”× ×ª×•× ×™× ×”×’×•×œ××™×™×...\")\n",
    "    \n",
    "    try:\n",
    "        data_dict, labels_dict = load_geophone_data()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('×“×•×’×××•×ª ××”× ×ª×•× ×™× ×”×’×•×œ××™×™× (2000 ×“×’×™××•×ª ×¨××©×•× ×•×ª)', fontsize=16)\n",
    "        \n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (filename, signal_data) in enumerate(data_dict.items()):\n",
    "            if i >= 4:\n",
    "                break\n",
    "            \n",
    "            # ×”×¦×’×ª 3 ×—×™×™×©× ×™× ×¨××©×•× ×™×\n",
    "            sample_data = signal_data[:2000, :min(3, signal_data.shape[1])]\n",
    "            \n",
    "            for j in range(sample_data.shape[1]):\n",
    "                axes[i].plot(sample_data[:, j], \n",
    "                           label=f'×—×™×™×©×Ÿ {j+1}', linewidth=1)\n",
    "            \n",
    "            axes[i].set_title(f'{labels_dict[filename]} - {filename}')\n",
    "            axes[i].set_xlabel('×–××Ÿ (×“×’×™××•×ª)')\n",
    "            axes[i].set_ylabel('×××¤×œ×™×˜×•×“×”')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # ×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "            mean_val = np.mean(np.abs(sample_data))\n",
    "            std_val = np.std(sample_data)\n",
    "            axes[i].text(0.02, 0.98, f'×××•×¦×¢ ××•×—×œ×˜: {mean_val:.4f}\\n×¡×˜×™×™×ª ×ª×§×Ÿ: {std_val:.4f}', \n",
    "                        transform=axes[i].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"×©×’×™××” ×‘×”×¦×’×ª × ×ª×•× ×™× ×’×•×œ××™×™×: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_preprocessing_comparison():\n",
    "    \"\"\"×”×©×•×•××” ×‘×™×Ÿ × ×ª×•× ×™× ×’×•×œ××™×™× ×œ××¢×•×‘×“×™×\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ ×”×©×•×•××” ×‘×™×Ÿ ×¢×™×‘×•×“ ×’×•×œ××™ ×œ××ª×§×“×...\")\n",
    "    \n",
    "    try:\n",
    "        data_dict, labels_dict = load_geophone_data()\n",
    "        \n",
    "        # ×œ×§×™×—×ª ×“×’×™××” ××¦×¢×“×™ ××“×\n",
    "        man_data = None\n",
    "        for filename, signal_data in data_dict.items():\n",
    "            if 'man' in filename.lower():\n",
    "                man_data = signal_data[:2000]  # 2 ×©× ×™×•×ª\n",
    "                break\n",
    "        \n",
    "        if man_data is None:\n",
    "            print(\"âŒ ×œ× × ××¦× ×§×•×‘×¥ ×¦×¢×“×™ ××“×\")\n",
    "            return\n",
    "        \n",
    "        # ×¢×™×‘×•×“ ××ª×§×“×\n",
    "        extractor = GeophoneFeatureExtractor()\n",
    "        mel_spec = extractor.extract_mel_spectrogram(man_data, channel=0)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('×”×©×•×•××”: × ×ª×•× ×™× ×’×•×œ××™×™× vs ××¢×•×‘×“×™×', fontsize=16)\n",
    "        \n",
    "        # × ×ª×•× ×™× ×’×•×œ××™×™× - ×—×™×™×©×Ÿ ×¨××©×•×Ÿ\n",
    "        axes[0, 0].plot(man_data[:, 0])\n",
    "        axes[0, 0].set_title('× ×ª×•× ×™× ×’×•×œ××™×™× - ×—×™×™×©×Ÿ 1')\n",
    "        axes[0, 0].set_xlabel('×–××Ÿ (×“×’×™××•×ª)')\n",
    "        axes[0, 0].set_ylabel('×××¤×œ×™×˜×•×“×”')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # FFT ×©×œ ×”× ×ª×•× ×™× ×”×’×•×œ××™×™×\n",
    "        fft_vals = fft(man_data[:, 0])\n",
    "        freqs = fftfreq(len(man_data), 1/1000)\n",
    "        axes[0, 1].plot(freqs[:len(freqs)//2], np.abs(fft_vals[:len(fft_vals)//2]))\n",
    "        axes[0, 1].set_title('FFT ×©×œ ×”× ×ª×•× ×™× ×”×’×•×œ××™×™×')\n",
    "        axes[0, 1].set_xlabel('×ª×“×¨ (×”×¨×¥)')\n",
    "        axes[0, 1].set_ylabel('×¢×•×¦××”')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # ×¡×¤×§×˜×•×’×¨××” ××ª×§×“××ª\n",
    "        librosa.display.specshow(mel_spec, sr=1000, hop_length=512, \n",
    "                                x_axis='time', y_axis='mel', ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('××œ-×¡×¤×§×˜×•×’×¨××” ××ª×§×“××ª')\n",
    "        \n",
    "        # ×”×©×•×•××ª ×××¤×™×™× ×™×\n",
    "        fft_features = extractor.extract_fft_features(man_data)\n",
    "        time_features = extractor.extract_time_domain_features(man_data)\n",
    "        \n",
    "        axes[1, 1].bar(['FFT Features', 'Time Features'], \n",
    "                      [len(fft_features), len(time_features)],\n",
    "                      color=['lightblue', 'lightgreen'])\n",
    "        axes[1, 1].set_title('××¡×¤×¨ ×××¤×™×™× ×™× ××—×•×œ×¦×™×')\n",
    "        axes[1, 1].set_ylabel('××¡×¤×¨ ×××¤×™×™× ×™×')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"ğŸ“Š ××¡×¤×¨ ×××¤×™×™× ×™ FFT: {len(fft_features)}\")\n",
    "        print(f\"â° ××¡×¤×¨ ×××¤×™×™× ×™× ×‘×–××Ÿ: {len(time_features)}\")\n",
    "        print(f\"ğŸ¨ ×¦×•×¨×ª ×¡×¤×§×˜×•×’×¨××”: {mel_spec.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"×©×’×™××” ×‘×”×©×•×•××ª ×¢×™×‘×•×“: {e}\")\n",
    "\n",
    "def generate_improvement_recommendations(results_list):\n",
    "    \"\"\"×™×¦×™×¨×ª ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨ ×”×‘×™×¦×•×¢×™×\"\"\"\n",
    "    \n",
    "    print(\"\\nğŸ¯ ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨ ×”×‘×™×¦×•×¢×™×\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not results_list:\n",
    "        print(\"âŒ ××™×Ÿ ×ª×•×¦××•×ª ×œ× ×™×ª×•×—\")\n",
    "        return\n",
    "    \n",
    "    # ××¦×™××ª ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨\n",
    "    best_result = max([r for r in results_list if r], key=lambda x: x['test_accuracy'])\n",
    "    worst_result = min([r for r in results_list if r], key=lambda x: x['test_accuracy'])\n",
    "    \n",
    "    print(f\"ğŸ† ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨: {best_result['type']} ({best_result['test_accuracy']:.4f})\")\n",
    "    print(f\"ğŸ“‰ ×”××•×“×œ ×”× ××•×š ×‘×™×•×ª×¨: {worst_result['type']} ({worst_result['test_accuracy']:.4f})\")\n",
    "    \n",
    "    improvement_gap = best_result['test_accuracy'] - worst_result['test_accuracy']\n",
    "    print(f\"ğŸ“ˆ ×¤×¢×¨ ×©×™×¤×•×¨: {improvement_gap:.4f} ({improvement_gap*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ ×”××œ×¦×•×ª ×¡×¤×¦×™×¤×™×•×ª:\")\n",
    "    \n",
    "    # ×”××œ×¦×•×ª ×¢×œ ×‘×¡×™×¡ ×”×ª×•×¦××•×ª\n",
    "    if best_result['test_accuracy'] < 0.7:\n",
    "        print(\"ğŸ”´ ×“×™×•×§ × ××•×š - ×”××œ×¦×•×ª ×—×™×¨×•×:\")\n",
    "        print(\"   â€¢ ×‘×“×•×§ ××™×›×•×ª ×”× ×ª×•× ×™× ×•× ×§×” ×¨×¢×©\")\n",
    "        print(\"   â€¢ ×”×’×“×œ ××ª ××§×‘×¥ ×”× ×ª×•× ×™×\")\n",
    "        print(\"   â€¢ × ×¡×” ×ª×›×•× ×•×ª × ×•×¡×¤×•×ª (MFCC, wavelet)\")\n",
    "        print(\"   â€¢ ×©×§×•×œ data augmentation\")\n",
    "    \n",
    "    elif best_result['test_accuracy'] < 0.85:\n",
    "        print(\"ğŸŸ¡ ×“×™×•×§ ×‘×™× ×•× ×™ - ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨:\")\n",
    "        print(\"   â€¢ ×›×•×•× ×Ÿ ×”×™×¤×¨-×¤×¨××˜×¨×™×\")\n",
    "        print(\"   â€¢ × ×¡×” ××¨×›×™×˜×§×˜×•×¨×•×ª ×©×•× ×•×ª\")\n",
    "        print(\"   â€¢ ×”×•×¡×£ regularization\")\n",
    "        print(\"   â€¢ ×©×¤×¨ ××ª ××™×›×•×ª ×”×××¤×™×™× ×™×\")\n",
    "    \n",
    "    else:\n",
    "        print(\"ğŸŸ¢ ×“×™×•×§ ×˜×•×‘ - ×”××œ×¦×•×ª ×œ××•×¤×˜×™××™×–×¦×™×”:\")\n",
    "        print(\"   â€¢ ××•×¤×˜×™××™×–×¦×™×” ×©×œ ××”×™×¨×•×ª ×”××•×“×œ\")\n",
    "        print(\"   â€¢ ×‘×“×™×§×ª robustness ×¢×œ × ×ª×•× ×™× ×—×“×©×™×\")\n",
    "        print(\"   â€¢ deploy ×•×‘×“×™×§×” ×‘×¡×‘×™×‘×” ×××™×ª×™×ª\")\n",
    "    \n",
    "    print(\"\\nğŸ”§ ×”××œ×¦×•×ª ×˜×›× ×™×•×ª ×›×œ×œ×™×•×ª:\")\n",
    "    print(\"   â€¢ × ×¡×” ensemble ×©×œ ××¡×¤×¨ ××•×“×œ×™×\")\n",
    "    print(\"   â€¢ ×©×§×•×œ transfer learning ××ª×—×•××™× ×“×•××™×\")\n",
    "    print(\"   â€¢ ×‘×¦×¢ cross-validation ××§×™×£\")\n",
    "    print(\"   â€¢ ×”×•×¡×£ ×”××©×š × ×™×˜×•×¨ ×‘×™×¦×•×¢×™×\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š ×”××œ×¦×•×ª ×œ× ×ª×•× ×™×:\")\n",
    "    print(\"   â€¢ ××¡×•×£ × ×ª×•× ×™× × ×•×¡×¤×™× ×‘××’×•×•×Ÿ ×ª× ××™×\")\n",
    "    print(\"   â€¢ ×©×¤×¨ ××ª ××™×›×•×ª ×”×ª×•×•×™×•×ª\")\n",
    "    print(\"   â€¢ ×”×•×¡×£ ×§×˜×’×•×¨×™×•×ª × ×•×¡×¤×•×ª ×× ×¨×œ×•×•× ×˜×™\")\n",
    "    print(\"   â€¢ ×‘×“×•×§ ××™×–×•×Ÿ ×‘×™×Ÿ ×”×§×˜×’×•×¨×™×•×ª\")\n",
    "\n",
    "# ×”×¨×¦×ª ×”×¤×¨×•×™×§×˜ ×”××œ×\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ ×¤×¨×•×™×§×˜ ×–×™×”×•×™ ×¦×¢×“×™ ××“× - ×”×©×•×•××” ××§×™×¤×”\")\n",
    "    print(\"×× ×ª×•× ×™× ×’×•×œ××™×™× ×œ×¢×™×‘×•×“ ××ª×§×“×\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ×”×¦×’×ª ×“×•×’×××•×ª ××”× ×ª×•× ×™×\n",
    "    print(\"\\nğŸ“Š ×”×¦×’×ª ×“×•×’×××•×ª ××”× ×ª×•× ×™× ×”×’×•×œ××™×™×...\")\n",
    "    visualize_raw_data_samples()\n",
    "    \n",
    "    # ×”×©×•×•××ª ×¢×™×‘×•×“ × ×ª×•× ×™×\n",
    "    print(\"\\nğŸ”¬ ×”×©×•×•××ª ×©×™×˜×•×ª ×¢×™×‘×•×“...\")\n",
    "    show_data_preprocessing_comparison()\n",
    "    \n",
    "    # ×”×¨×¦×ª ×”×©×•×•××” ××œ××”\n",
    "    print(\"\\nğŸƒâ€â™‚ï¸ ×”×¨×¦×ª ×”×©×•×•××” ××œ××”...\")\n",
    "    results = run_full_comparison()\n",
    "    \n",
    "    # ×•×™×–×•××œ×™×–×¦×™×” ××ª×§×“××ª\n",
    "    if results:\n",
    "        print(\"\\nğŸ“ˆ ×•×™×–×•××œ×™×–×¦×™×” ××ª×§×“××ª...\")\n",
    "        plot_model_comparison_detailed(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
