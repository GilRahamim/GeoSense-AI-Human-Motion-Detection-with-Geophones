# -*- coding: utf-8 -*-
# =============================================================================
# Full Pipeline: Geophone Data Analysis, Preprocessing, Spectrograms, and CNN Training
# =============================================================================

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import glob
from tqdm import tqdm
from scipy import signal as sp_signal
import itertools
import math
import random
import time

# Deep Learning Imports
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns

print("--- Starting Full Pipeline ---")

# =============================================================================
# --- 1. תצורה (Configuration) ---
# =============================================================================
DATA_DIR = 'data'                  # Directory containing the CSV files
SAMPLING_RATE = 1000               # Hz (Sampling rate of the geophones)
PROBLEM_CHANNELS = {0}             # Set of problematic channel indices (identified previously)
PLOT_SAVE_DIR = "output_plots"     # Directory to save analysis plots
MODEL_SAVE_PATH = "best_cnn_model.pth" # Path to save the best trained model

# Windowing Parameters
WINDOW_DURATION_SEC = 5            # Length of each window in seconds
HOP_RATIO = 0.5                    # Percentage overlap between windows (0.5 = 50%)
WINDOW_SIZE_SAMPLES = int(WINDOW_DURATION_SEC * SAMPLING_RATE)
HOP_SIZE_SAMPLES = int(WINDOW_SIZE_SAMPLES * HOP_RATIO)

# Spectrogram Parameters
NPERSEG = 256                      # Length of each segment (FFT window)
NOVERLAP_RATIO = 0.75              # Overlap ratio for spectrogram windows
NOVERLAP = int(NPERSEG * NOVERLAP_RATIO)
WINDOW = 'hann'                    # Window type
SCALING = 'density'                # 'density' for PSD, 'spectrum' for STFT magnitude
MODE = 'psd'                       # Output mode ('psd', 'magnitude')
EPSILON = 1e-10                    # Small value to prevent log(0)

# Training Parameters
SEED = 42                          # For reproducibility
TEST_SIZE = 0.15                   # Proportion for the Test set
VALIDATION_SIZE = 0.15             # Proportion for the Validation set (from non-Test data)
BATCH_SIZE = 32                    # Number of samples per batch during training
LEARNING_RATE = 0.0005             # Initial learning rate for the optimizer
NUM_EPOCHS = 50                    # Maximum number of training epochs
PATIENCE = 10                      # Early stopping patience (epochs without improvement)
SHOW_ANALYSIS_PLOTS = False        # Set to True to show correlation plots interactively (can be many!)

print(f"--- 1. Project Configuration ---")
print(f"Data Directory: {DATA_DIR}")
print(f"Sampling Rate: {SAMPLING_RATE} Hz")
print(f"Problematic Channels Ignored: {PROBLEM_CHANNELS}")
print(f"Window Duration: {WINDOW_DURATION_SEC} sec ({WINDOW_SIZE_SAMPLES} samples)")
print(f"Window Hop Size: {HOP_SIZE_SAMPLES} samples ({HOP_RATIO*100:.0f}% overlap)")
print(f"Spectrogram nperseg: {NPERSEG}, noverlap: {NOVERLAP}")
print(f"Train/Val/Test Split: Train=~{1-TEST_SIZE-VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Val=~{VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Test=~{TEST_SIZE:.0%}")
print(f"Batch Size: {BATCH_SIZE}, LR: {LEARNING_RATE}, Epochs: {NUM_EPOCHS}, Patience: {PATIENCE}")
print("-" * 30)

# Create directories if they don't exist
os.makedirs(PLOT_SAVE_DIR, exist_ok=True)

# Set Seed for reproducibility
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# =============================================================================
# --- 2. פונקציות עזר (Helper Functions) ---
# =============================================================================
print("\n--- 2. Defining Helper Functions ---")

def get_label_from_filename(filename):
    """Extracts label (category) from filename."""
    filename_lower = filename.lower()
    if 'nothing' in filename_lower: return 'nothing'
    elif 'human' in filename_lower or 'man' in filename_lower: return 'human'
    elif 'car' in filename_lower: return 'car'
    else: return None

def load_data(data_dir):
    """Loads CSV data from the directory and organizes it."""
    data_structured = {}
    labels_by_filename = {}
    all_source_keys = []
    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))
    print(f"\n--- 2a. Loading Data ---")
    print(f"Found {len(csv_files)} CSV files in '{data_dir}'.")
    if not csv_files: print("Error: No CSV files found."); exit()
    for filepath in tqdm(csv_files, desc="Loading Files", unit="file", leave=False):
        filename = os.path.basename(filepath)
        label = get_label_from_filename(filename)
        if label is None: continue
        try:
            if os.path.getsize(filepath) == 0: continue
            df = pd.read_csv(filepath, header=None)
            if df.empty: continue
            data_structured[filename] = {}
            labels_by_filename[filename] = label
            for i, col_name in enumerate(df.columns):
                if pd.api.types.is_numeric_dtype(df[col_name]):
                    sensor_series = df[col_name].fillna(0).values.astype(np.float64)
                    if len(sensor_series) >= 10 and np.std(sensor_series) > 1e-9:
                         data_structured[filename][i] = sensor_series
                         all_source_keys.append((filename, i))
        except Exception as e: tqdm.write(f"Error processing {filename}: {e}")
    print(f"\nFinished loading. Total unique signals (channels) loaded: {len(all_source_keys)}")
    if not all_source_keys: print("Error: No valid signals loaded."); exit()
    print("\nSummary of Loaded Data:")
    for fname, cols_dict in data_structured.items(): print(f"- File: {fname} (Label: {labels_by_filename[fname]}), Columns: {list(cols_dict.keys())}")
    print("-" * 30)
    return data_structured, labels_by_filename, all_source_keys

def calculate_cross_correlation(sig1, sig2, sampling_rate):
    """Calculates normalized cross-correlation and peak info."""
    sig1 = np.asarray(sig1, dtype=np.float64); sig2 = np.asarray(sig2, dtype=np.float64)
    sig1 = np.nan_to_num(sig1); sig2 = np.nan_to_num(sig2)
    if len(sig1) < 2 or len(sig2) < 2: return None, None, None, None, None
    mean1, std1 = np.mean(sig1), np.std(sig1); mean2, std2 = np.mean(sig2), np.std(sig2)
    if std1 < 1e-9 or std2 < 1e-9 : n1, n2 = len(sig1), len(sig2); lags_samples = np.arange(-(n2 - 1), n1); lags_ms = lags_samples * (1000.0 / sampling_rate); return lags_ms, np.zeros(len(lags_ms)), 0.0, 0.0, 0
    sig1_norm = (sig1 - mean1) / std1; sig2_norm = (sig2 - mean2) / std2
    correlation = np.correlate(sig1_norm, sig2_norm, mode='full')
    n1, n2 = len(sig1_norm), len(sig2_norm); lags_samples = np.arange(-(n2 - 1), n1)
    lags_ms = lags_samples * (1000.0 / sampling_rate)
    norm_factor = np.sqrt(np.sum(sig1_norm**2) * np.sum(sig2_norm**2))
    if norm_factor < 1e-9: normalized_correlation = np.zeros_like(correlation); else: normalized_correlation = correlation / norm_factor
    if len(normalized_correlation) == 0: return None, None, None, None, None
    peak_index = np.argmax(np.abs(normalized_correlation)); peak_lag_ms = lags_ms[peak_index]; peak_value = normalized_correlation[peak_index]
    peak_lag_samples = int(round(peak_lag_ms * sampling_rate / 1000.0)) if peak_lag_ms is not None else 0
    if peak_index < 0 or peak_index >= len(lags_ms): peak_lag_ms, peak_value, peak_lag_samples = None, None, 0
    return lags_ms, normalized_correlation, peak_lag_ms, peak_value, peak_lag_samples

def plot_cross_correlation(lags_ms, correlation, peak_lag_ms, peak_value, title, filename_to_save=None):
    """Plots cross-correlation, optionally saves to file."""
    if lags_ms is None or correlation is None: return
    fig, ax = plt.subplots(figsize=(12, 5))
    ax.plot(lags_ms, correlation, label='Cross-correlation value', linewidth=1.5)
    if peak_lag_ms is not None and peak_value is not None: ax.scatter([peak_lag_ms], [peak_value], color='red', s=100, zorder=5, label=f'Peak: {peak_value:.2f} at {peak_lag_ms:.1f} ms'); else: ax.text(0.05, 0.9, "Peak info unavailable", transform=ax.transAxes, color='red')
    ax.axhline(0, color='grey', linestyle='--', linewidth=0.7); ax.axvline(0, color='grey', linestyle='--', linewidth=0.7)
    ax.set_title(title, fontsize=11); ax.set_xlabel('Lag (ms)', fontsize=10); ax.set_ylabel('Norm. Cross-corr [-1, 1]', fontsize=10)
    ax.set_ylim([-1.1, 1.1]); ax.legend(); ax.grid(True, linestyle=':'); plt.tight_layout()
    if filename_to_save:
        try: plt.savefig(filename_to_save); print(f"  Saved plot: {filename_to_save}")
        except Exception as e: print(f"  Error saving plot {filename_to_save}: {e}")
        plt.close(fig) # Close after saving
    elif SHOW_ANALYSIS_PLOTS:
        plt.show() # Show interactively only if requested
    else:
        plt.close(fig) # Close if not saving and not showing interactively

def align_signal(signal_to_align, n_ref, lag_samples):
    """Aligns a signal based on a given lag relative to a reference length."""
    n_align = len(signal_to_align); aligned_signal = np.zeros(n_ref)
    if lag_samples > 0: len_to_copy = min(n_ref, n_align - lag_samples); if len_to_copy > 0: aligned_signal[:len_to_copy] = signal_to_align[lag_samples:lag_samples + len_to_copy]
    elif lag_samples < 0: start_index_aligned = abs(lag_samples); len_to_copy = min(n_align, n_ref - start_index_aligned); if len_to_copy > 0: aligned_signal[start_index_aligned:start_index_aligned + len_to_copy] = signal_to_align[:len_to_copy]
    else: len_to_copy = min(n_ref, n_align); aligned_signal[:len_to_copy] = signal_to_align[:len_to_copy]
    return aligned_signal

print("Helper functions defined.")
print("-" * 30)

# --- 3. ניתוח קורלציה וזיהוי עמודות (Correlation Analysis & Channel Selection) ---
# (הרצת ניתוח קורלציה מלא לתיעוד, אבל לא נשתמש בכל הגרפים)
print("\n--- 3. Correlation Analysis (Identifying Problematic Channels) ---")
data_structured, labels_by_filename, all_source_keys = load_data(DATA_DIR)

# Perform all-pairs comparison to document findings
all_comparisons = list(itertools.combinations_with_replacement(all_source_keys, 2))
print(f"Performing {len(all_comparisons)} cross-correlation comparisons for analysis...")
results = {}
problematic_correlations_summary = {} # Store problematic correlations for summary

for key1, key2 in tqdm(all_comparisons, desc="Analyzing Correlations", leave=False):
    file1, col1 = key1; file2, col2 = key2
    if file1 not in data_structured or col1 not in data_structured[file1] or \
       file2 not in data_structured or col2 not in data_structured[file2]: continue
    signal1 = data_structured[file1][col1]; signal2 = data_structured[file2][col2]
    if len(signal1) < 2 or len(signal2) < 2: continue

    lags, corr, peak_lag, peak_val, _ = calculate_cross_correlation(signal1, signal2, SAMPLING_RATE)
    if peak_val is not None: results[(key1, key2)] = peak_val

    # בדיקת בעיות ספציפית בין אירוע לשקט באותה עמודה
    label1 = labels_by_filename.get(file1); label2 = labels_by_filename.get(file2)
    if key1 == key2: continue # Skip self-comparison for problem check
    if col1 == col2 and label1 is not None and label2 is not None and label1 != 'nothing' and label2 == 'nothing':
        if abs(peak_val) > PROBLEM_THRESHOLD:
            problematic_correlations_summary[(key1, key2)] = peak_val
            PROBLEM_CHANNELS.add(col1) # Update problematic channels set

    # Optional: Plot and save *this specific problematic* correlation for documentation
    # if abs(peak_val) > PROBLEM_THRESHOLD and col1 == col2 and label1 != label2 and (label1=='nothing' or label2=='nothing'):
    #     safe_title = f"Problem_Corr_{label1}_vs_{label2}_Col{col1}".replace(' ','_')
    #     filename = os.path.join(PLOT_SAVE_DIR, f"{safe_title}.png")
    #     plot_title = f"High Correlation ({peak_val:.2f})\n{label1}_Col{col1} vs {label2}_Col{col1}"
    #     plot_cross_correlation(lags, corr, peak_lag, peak_val, plot_title, filename_to_save=filename)

print("\nCorrelation analysis complete.")
if problematic_correlations_summary:
     print(f"\n--- Identified Problematic Correlations (Event vs Nothing > {PROBLEM_THRESHOLD}) ---")
     sorted_problems = sorted(problematic_correlations_summary.items(), key=lambda item: abs(item[1]), reverse=True)
     for (key1, key2), peak_val in sorted_problems:
         file1, col1 = key1; file2, col2 = key2; label1 = labels_by_filename.get(file1,'?'); label2 = labels_by_filename.get(file2,'?')
         print(f"*   Peak={peak_val:.2f} between [{label1}@{file1}_Col{col1}] and [{label2}@{file2}_Col{col2}]")
     print(f"Updated Problematic Channels: {PROBLEM_CHANNELS}")
else:
     print("\nNo significantly high correlations found between events and 'nothing' on the same channel.")

all_available_channels = set(col for _, col in all_source_keys)
good_channels = sorted(list(all_available_channels - PROBLEM_CHANNELS))
print(f"\nPotentially Good Channels identified for use: {good_channels}")
if not good_channels: print("Error: No good channels identified!"); exit()
print("-" * 30)

# --- 4. מיצוע מיושר (Aligned Averaging on Good Channels) ---
print("\n--- 4. Performing Aligned Averaging on Good Channels ---")
averaged_signals = {}
processed_files_count = 0
files_to_process = list(data_structured.keys())
for filename in tqdm(files_to_process, desc="Averaging Files"):
    if filename not in data_structured: continue
    label = labels_by_filename[filename]
    available_cols = sorted(list(data_structured[filename].keys()))
    good_cols_for_file = [col for col in available_cols if col not in PROBLEM_CHANNELS] # Use only good channels

    if len(good_cols_for_file) < 1: continue # Skip if no good channels for this file
    elif len(good_cols_for_file) == 1:
        averaged_signal = data_structured[filename][good_cols_for_file[0]]
    else:
        ref_col_idx = good_cols_for_file[0]; signal_ref = data_structured[filename][ref_col_idx]; n_ref = len(signal_ref)
        sum_aligned_signals = np.copy(signal_ref); num_signals_averaged = 1
        for i in range(1, len(good_cols_for_file)):
            col_to_align_idx = good_cols_for_file[i]; signal_to_align = data_structured[filename][col_to_align_idx]
            _, _, _, _, peak_lag_samples = calculate_cross_correlation(signal_ref, signal_to_align, SAMPLING_RATE)
            if peak_lag_samples is None: continue
            aligned_signal = align_signal(signal_to_align, n_ref, peak_lag_samples)
            sum_aligned_signals += aligned_signal; num_signals_averaged += 1
        if num_signals_averaged > 0: averaged_signal = sum_aligned_signals / num_signals_averaged
        else: averaged_signal = signal_ref # Fallback
    if label not in averaged_signals: averaged_signals[label] = []
    averaged_signals[label].append(averaged_signal); processed_files_count += 1
print(f"\nFinished averaging using {len(good_channels)} good channels per file (where available). Processed {processed_files_count} files.")
print("-" * 30)

# --- 5. חלוקה לחלונות (Windowing Averaged Signals) ---
print("\n--- 5. Windowing Averaged Signals ---")
all_windows = []; all_window_labels = []
labels_to_process = list(averaged_signals.keys())
for label in tqdm(labels_to_process, desc="Windowing Labels", leave=False):
    for avg_signal in averaged_signals[label]:
        num_samples = len(avg_signal)
        if num_samples < WINDOW_SIZE_SAMPLES:
             padded_signal = np.pad(avg_signal, (0, WINDOW_SIZE_SAMPLES - num_samples), 'constant')
             all_windows.append(padded_signal); all_window_labels.append(label)
        else:
            num_windows = math.floor((num_samples - WINDOW_SIZE_SAMPLES) / HOP_SIZE_SAMPLES) + 1
            for i in range(num_windows):
                start_idx = i * HOP_SIZE_SAMPLES; end_idx = start_idx + WINDOW_SIZE_SAMPLES
                window = avg_signal[start_idx:end_idx]; all_windows.append(window); all_window_labels.append(label)
print(f"\nFinished windowing. Total windows created: {len(all_windows)}")
if not all_windows: print("Error: No windows created."); exit()
print("Window counts per label:")
unique_labels, counts = np.unique(all_window_labels, return_counts=True)
for l, c in zip(unique_labels, counts): print(f"- {l}: {c} windows")
print("-" * 30)

# --- 6. יצירת ספקטוגרמות (Generating Spectrograms) ---
print("\n--- 6. Generating Spectrograms from Windows ---")
all_spectrograms = []; all_spec_labels = all_window_labels
for window_signal in tqdm(all_windows, desc="Generating Spectrograms"):
    try:
        frequencies, times, Sxx = sp_signal.spectrogram(window_signal, fs=SAMPLING_RATE, window=WINDOW, nperseg=NPERSEG, noverlap=NOVERLAP, scaling=SCALING, mode=MODE)
        log_Sxx = 10 * np.log10(Sxx + EPSILON); all_spectrograms.append(log_Sxx.astype(np.float32))
    except Exception as e: tqdm.write(f"Error generating spectrogram: {e}")
print(f"\nGenerated {len(all_spectrograms)} spectrograms.")
if not all_spectrograms: print("Error: No spectrograms generated."); exit()
print("-" * 30)

# --- 7. הכנת נתונים לאימון (Data Preparation for Training) ---
print("\n--- 7. Preparing Data for Training ---")
label_encoder = LabelEncoder(); encoded_labels = label_encoder.fit_transform(all_spec_labels)
num_classes = len(label_encoder.classes_); class_names = list(label_encoder.classes_)
print(f"Label encoding: {list(zip(class_names, range(num_classes)))}")
spectrograms_with_channel = np.array(all_spectrograms)[:, np.newaxis, :, :]; print(f"Spectrogram shape: {spectrograms_with_channel.shape}")
X = spectrograms_with_channel; y = encoded_labels
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y)
val_size_adjusted = VALIDATION_SIZE / (1 - TEST_SIZE)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size_adjusted, random_state=SEED, stratify=y_train_val)
print(f"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")

class SpectrogramDataset(Dataset):
    def __init__(self, features, labels): self.features = torch.from_numpy(features).float(); self.labels = torch.from_numpy(labels).long()
    def __len__(self): return len(self.features)
    def __getitem__(self, idx): return self.features[idx], self.labels[idx]

train_dataset = SpectrogramDataset(X_train, y_train); val_dataset = SpectrogramDataset(X_val, y_val); test_dataset = SpectrogramDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
print("PyTorch Datasets and DataLoaders created.")
print("-" * 30)

# --- 8. הגדרת מודל CNN (CNN Model Definition) ---
print("\n--- 8. Defining CNN Model ---")
class SimpleCNN(nn.Module):
    def __init__(self, num_classes, input_height, input_width):
        super(SimpleCNN, self).__init__()
        self.conv_block1 = nn.Sequential(nn.Conv2d(1, 16, 3, 1, 1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv_block2 = nn.Sequential(nn.Conv2d(16, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2))
        self.conv_block3 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2))
        # Calculate flattened size dynamically
        dummy_input = torch.randn(1, 1, input_height, input_width)
        dummy_output = self.conv_block3(self.conv_block2(self.conv_block1(dummy_input)))
        self.flattened_size = int(np.prod(dummy_output.shape))

        self.fc_block = nn.Sequential(nn.Flatten(), nn.Linear(self.flattened_size, 128), nn.ReLU(), nn.Dropout(0.5), nn.Linear(128, num_classes))
    def forward(self, x): x = self.conv_block1(x); x = self.conv_block2(x); x = self.conv_block3(x); x = self.fc_block(x); return x

spec_height = spectrograms_with_channel.shape[2]; spec_width = spectrograms_with_channel.shape[3]
model = SimpleCNN(num_classes=num_classes, input_height=spec_height, input_width=spec_width)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu"); model.to(device)
print(f"Model defined and moved to {device}.")
# print(model) # Uncomment to print model structure
print("-" * 30)

# --- 9. הגדרת אימון (Training Setup) ---
print("\n--- 9. Setting up Training ---")
criterion = nn.CrossEntropyLoss(); optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)
print("Loss: CrossEntropyLoss, Optimizer: Adam, Scheduler: StepLR")
print("-" * 30)

# --- 10. לולאת אימון ואימות (Training and Validation Loop) ---
print("\n--- 10. Starting Training ---")
history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
best_val_accuracy = 0.0; epochs_no_improve = 0; training_start_time = time.time()

for epoch in range(NUM_EPOCHS):
    epoch_start_time = time.time(); model.train(); running_loss = 0.0; correct_train = 0; total_train = 0
    train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]", leave=False)
    for inputs, labels in train_pbar:
        inputs, labels = inputs.to(device), labels.to(device); optimizer.zero_grad()
        outputs = model(inputs); loss = criterion(outputs, labels); loss.backward(); optimizer.step()
        running_loss += loss.item() * inputs.size(0); _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0); correct_train += (predicted == labels).sum().item()
        train_pbar.set_postfix({'Loss': loss.item()})
    epoch_train_loss = running_loss / len(train_loader.dataset); epoch_train_acc = 100.0 * correct_train / total_train
    history['train_loss'].append(epoch_train_loss); history['train_acc'].append(epoch_train_acc)

    model.eval(); running_val_loss = 0.0; correct_val = 0; total_val = 0
    val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]", leave=False)
    with torch.no_grad():
        for inputs, labels in val_pbar:
            inputs, labels = inputs.to(device), labels.to(device); outputs = model(inputs); loss = criterion(outputs, labels)
            running_val_loss += loss.item() * inputs.size(0); _, predicted = torch.max(outputs.data, 1)
            total_val += labels.size(0); correct_val += (predicted == labels).sum().item()
            val_pbar.set_postfix({'Loss': loss.item()})
    epoch_val_loss = running_val_loss / len(val_loader.dataset); epoch_val_acc = 100.0 * correct_val / total_val
    history['val_loss'].append(epoch_val_loss); history['val_acc'].append(epoch_val_acc)

    current_lr = optimizer.param_groups[0]['lr']; scheduler.step(); epoch_duration = time.time() - epoch_start_time
    print(f"Epoch {epoch+1}/{NUM_EPOCHS} [{epoch_duration:.1f}s] - LR: {current_lr:.1e} - Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.2f}% - Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.2f}%")

    if epoch_val_acc > best_val_accuracy:
        print(f"  Validation accuracy improved ({best_val_accuracy:.2f}% -> {epoch_val_acc:.2f}%). Saving model to {MODEL_SAVE_PATH}...")
        best_val_accuracy = epoch_val_acc; torch.save(model.state_dict(), MODEL_SAVE_PATH); epochs_no_improve = 0
    else:
        epochs_no_improve += 1; print(f"  Val acc did not improve for {epochs_no_improve} epoch(s). Best: {best_val_accuracy:.2f}%")
    if epochs_no_improve >= PATIENCE: print(f"\nEarly stopping triggered after {epoch+1} epochs."); break

training_duration = time.time() - training_start_time
print(f"\n--- Training Finished ({training_duration / 60:.1f} minutes) ---"); print(f"Best Validation Accuracy: {best_val_accuracy:.2f}%"); print("-" * 30)

# --- 11. הערכה סופית (Final Evaluation on Test Set) ---
print("\n--- 11. Evaluating on Test Set ---")
if not os.path.exists(MODEL_SAVE_PATH):
     print(f"Error: Best model file not found at {MODEL_SAVE_PATH}. Skipping test evaluation.")
else:
    print(f"Loading best model from {MODEL_SAVE_PATH}..."); model.load_state_dict(torch.load(MODEL_SAVE_PATH)); model.to(device); model.eval()
    all_preds = []; all_labels = []
    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Testing"):
            inputs, labels = inputs.to(device), labels.to(device); outputs = model(inputs); _, predicted = torch.max(outputs.data, 1)
            all_preds.extend(predicted.cpu().numpy()); all_labels.extend(labels.cpu().numpy())
    test_accuracy = accuracy_score(all_labels, all_preds); print(f"\nTest Accuracy: {test_accuracy * 100.0:.2f}%")
    print("\nClassification Report:"); print(classification_report(all_labels, all_preds, target_names=class_names))
    print("\nConfusion Matrix:"); cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label'); plt.ylabel('True Label'); plt.title('Confusion Matrix - Test Set'); plt.show()
print("-" * 30)

# --- 12. הצגת גרפי אימון (Plotting Training Curves) ---
print("\n--- 12. Plotting Training Curves ---")
epochs_range = range(1, len(history['train_loss']) + 1)
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1); plt.plot(epochs_range, history['train_loss'], label='Training Loss'); plt.plot(epochs_range, history['val_loss'], label='Validation Loss'); plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend(); plt.grid(True)
plt.subplot(1, 2, 2); plt.plot(epochs_range, history['train_acc'], label='Training Accuracy'); plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy'); plt.xlabel('Epochs'); plt.ylabel('Accuracy (%)'); plt.title('Accuracy'); plt.legend(); plt.grid(True)
plt.suptitle('Training and Validation Metrics')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

print("\n--- Full Pipeline Complete ---")
