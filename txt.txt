import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import glob
from tqdm import tqdm
from scipy import signal as sp_signal
import itertools
import math # Need math for ceil

# --- 1. תצורה (Configuration) ---
DATA_DIR = 'data'
SAMPLING_RATE = 1000 # Hz
PROBLEM_CHANNELS = {0} # הגדרנו שעמודה 0 היא הבעייתית היחידה

# פרמטרים לחלוקה לחלונות (Windowing)
WINDOW_DURATION_SEC = 5 # אורך כל חלון בשניות
HOP_RATIO = 0.5         # אחוז חפיפה בין חלונות (0.5 = 50%)
WINDOW_SIZE_SAMPLES = int(WINDOW_DURATION_SEC * SAMPLING_RATE)
HOP_SIZE_SAMPLES = int(WINDOW_SIZE_SAMPLES * HOP_RATIO)

# פרמטרים ליצירת ספקטוגרמה
NPERSEG = 256
NOVERLAP_RATIO = 0.75
NOVERLAP = int(NPERSEG * NOVERLAP_RATIO)
WINDOW = 'hann'
SCALING = 'density'
MODE = 'psd'
epsilon = 1e-10 # למניעת log(0)

print(f"--- Project Configuration ---")
print(f"Data Directory: {DATA_DIR}")
print(f"Sampling Rate: {SAMPLING_RATE} Hz")
print(f"Problematic Channels Identified: {PROBLEM_CHANNELS}")
print(f"Window Duration: {WINDOW_DURATION_SEC} sec ({WINDOW_SIZE_SAMPLES} samples)")
print(f"Window Hop Size: {HOP_SIZE_SAMPLES} samples ({HOP_RATIO*100:.0f}% overlap)")
print(f"Spectrogram nperseg: {NPERSEG}, noverlap: {NOVERLAP}")
print("-" * 30)

# --- 2. פונקציות עזר (כמו קודם) ---
def get_label_from_filename(filename):
    filename_lower = filename.lower()
    if 'nothing' in filename_lower: return 'nothing'
    elif 'human' in filename_lower or 'man' in filename_lower: return 'human'
    elif 'car' in filename_lower: return 'car'
    else: return None

def load_data(data_dir):
    # ... (פונקציית הטעינה זהה לקוד הקודם) ...
    data_structured = {}
    labels_by_filename = {}
    all_source_keys = []
    csv_files = glob.glob(os.path.join(data_dir, '*.csv'))
    print(f"\n--- 2a. Loading Data ---")
    print(f"Found {len(csv_files)} CSV files in '{data_dir}'.")
    if not csv_files: print("Error: No CSV files found."); exit()
    for filepath in tqdm(csv_files, desc="Loading Files", unit="file"):
        filename = os.path.basename(filepath)
        label = get_label_from_filename(filename)
        if label is None: continue
        try:
            if os.path.getsize(filepath) == 0: continue
            df = pd.read_csv(filepath, header=None)
            if df.empty: continue
            data_structured[filename] = {}
            labels_by_filename[filename] = label
            for i, col_name in enumerate(df.columns):
                if pd.api.types.is_numeric_dtype(df[col_name]):
                    sensor_series = df[col_name].fillna(0).values.astype(np.float64)
                    if len(sensor_series) >= 10 and np.std(sensor_series) > 1e-9:
                         data_structured[filename][i] = sensor_series
                         all_source_keys.append((filename, i))
        except Exception as e: tqdm.write(f"Error processing {filename}: {e}")
    print(f"\nFinished loading. Total unique signals (channels) loaded: {len(all_source_keys)}")
    if not all_source_keys: print("Error: No valid signals loaded."); exit()
    print("\nSummary of Loaded Data:")
    for fname, cols_dict in data_structured.items(): print(f"- File: {fname} (Label: {labels_by_filename[fname]}), Columns: {list(cols_dict.keys())}")
    print("-" * 30)
    return data_structured, labels_by_filename, all_source_keys

def calculate_cross_correlation(sig1, sig2, sampling_rate):
    # ... (הפונקציה זהה, מחזירה גם peak_lag_samples) ...
    sig1 = np.asarray(sig1, dtype=np.float64); sig2 = np.asarray(sig2, dtype=np.float64)
    sig1 = np.nan_to_num(sig1); sig2 = np.nan_to_num(sig2)
    if len(sig1) < 2 or len(sig2) < 2: return None, None, None, None, None
    mean1, std1 = np.mean(sig1), np.std(sig1); mean2, std2 = np.mean(sig2), np.std(sig2)
    if std1 < 1e-9 or std2 < 1e-9 :
         n1, n2 = len(sig1), len(sig2); lags_samples = np.arange(-(n2 - 1), n1)
         lags_ms = lags_samples * (1000.0 / sampling_rate)
         return lags_ms, np.zeros(len(lags_ms)), 0.0, 0.0, 0
    sig1_norm = (sig1 - mean1) / std1; sig2_norm = (sig2 - mean2) / std2
    correlation = np.correlate(sig1_norm, sig2_norm, mode='full')
    n1, n2 = len(sig1_norm), len(sig2_norm); lags_samples = np.arange(-(n2 - 1), n1)
    lags_ms = lags_samples * (1000.0 / sampling_rate)
    norm_factor = np.sqrt(np.sum(sig1_norm**2) * np.sum(sig2_norm**2))
    if norm_factor < 1e-9: normalized_correlation = np.zeros_like(correlation)
    else: normalized_correlation = correlation / norm_factor
    if len(normalized_correlation) == 0: return None, None, None, None, None
    peak_index = np.argmax(np.abs(normalized_correlation))
    peak_lag_ms = lags_ms[peak_index]; peak_value = normalized_correlation[peak_index]
    peak_lag_samples = int(round(peak_lag_ms * sampling_rate / 1000.0)) if peak_lag_ms is not None else 0
    if peak_index < 0 or peak_index >= len(lags_ms): peak_lag_ms, peak_value, peak_lag_samples = None, None, 0
    return lags_ms, normalized_correlation, peak_lag_ms, peak_value, peak_lag_samples

def align_signal(signal_to_align, n_ref, lag_samples):
    """מיישר אות לפי lag נתון לאורך ייחוס n_ref."""
    n_align = len(signal_to_align)
    aligned_signal = np.zeros(n_ref)
    if lag_samples > 0: # הגיע מאוחר, הזז שמאלה
        len_to_copy = min(n_ref, n_align - lag_samples)
        if len_to_copy > 0: aligned_signal[:len_to_copy] = signal_to_align[lag_samples:lag_samples + len_to_copy]
    elif lag_samples < 0: # הגיע מוקדם, הזז ימינה
        start_index_aligned = abs(lag_samples)
        len_to_copy = min(n_align, n_ref - start_index_aligned)
        if len_to_copy > 0: aligned_signal[start_index_aligned:start_index_aligned + len_to_copy] = signal_to_align[:len_to_copy]
    else: # אין הזזה
        len_to_copy = min(n_ref, n_align)
        aligned_signal[:len_to_copy] = signal_to_align[:len_to_copy]
    return aligned_signal

# --- 3. מיצוע מיושר (Aligned Averaging) ---
print("\n--- 3. Performing Aligned Averaging on Good Channels ---")
data_structured, labels_by_filename, _ = load_data(DATA_DIR) # טען שוב או השתמש במה שנטען
averaged_signals = {} # מילון לשמור את האותות הממוצעים: key=label, value=list of averaged signals
processed_files_count = 0

files_to_process = list(data_structured.keys())
for filename in tqdm(files_to_process, desc="Averaging Files"):
    if filename not in data_structured: continue # Skip if file had issues loading

    label = labels_by_filename[filename]
    available_cols = sorted(list(data_structured[filename].keys()))

    # מצא את העמודות ה"טובות" עבור הקובץ הזה
    good_cols_for_file = [col for col in available_cols if col not in PROBLEM_CHANNELS]

    if len(good_cols_for_file) < 1:
        # tqdm.write(f"Skipping {filename}: No 'good' channels found.")
        continue
    elif len(good_cols_for_file) == 1:
        # אם יש רק עמודה טובה אחת, השתמש בה ישירות (אין מה למצע)
        tqdm.write(f"Info: Using single good channel {good_cols_for_file[0]} for {filename}.")
        averaged_signal = data_structured[filename][good_cols_for_file[0]]
    else:
        # בצע מיצוע מיושר
        ref_col_idx = good_cols_for_file[0] # בחר את העמודה הטובה הראשונה כרפרנס
        signal_ref = data_structured[filename][ref_col_idx]
        n_ref = len(signal_ref)
        sum_aligned_signals = np.copy(signal_ref) # התחל סכום עם הרפרנס
        num_signals_averaged = 1

        for i in range(1, len(good_cols_for_file)):
            col_to_align_idx = good_cols_for_file[i]
            signal_to_align = data_structured[filename][col_to_align_idx]

            # חשב קורלציה מול הרפרנס כדי למצוא lag
            _, _, _, _, peak_lag_samples = calculate_cross_correlation(signal_ref, signal_to_align, SAMPLING_RATE)

            if peak_lag_samples is None:
                # tqdm.write(f"Warning: Could not find lag for {filename} Col {ref_col_idx} vs {col_to_align_idx}. Skipping this channel for average.")
                continue

            # יישר את האות הנוכחי
            aligned_signal = align_signal(signal_to_align, n_ref, peak_lag_samples)
            sum_aligned_signals += aligned_signal
            num_signals_averaged += 1

        # חשב ממוצע סופי
        if num_signals_averaged > 0:
            averaged_signal = sum_aligned_signals / num_signals_averaged
        else: # אמור לא לקרות אם התחלנו עם הרפרנס, אבל למקרה ש...
             tqdm.write(f"Warning: Could not average any signals for {filename}. Using reference.")
             averaged_signal = signal_ref # Fallback to reference signal

    # שמור את האות הממוצע במילון לפי התווית
    if label not in averaged_signals:
        averaged_signals[label] = []
    averaged_signals[label].append(averaged_signal)
    processed_files_count += 1

print(f"\nFinished averaging. Processed {processed_files_count} files.")
print("Number of averaged signals per label:")
for label, signals in averaged_signals.items():
    print(f"- {label}: {len(signals)} signals")
print("-" * 30)


# --- 4. חלוקה לחלונות (Windowing / Chunking) ---
print("\n--- 4. Windowing Averaged Signals ---")
all_windows = []
all_window_labels = []

labels_to_process = list(averaged_signals.keys())
for label in tqdm(labels_to_process, desc="Windowing Labels"):
    for avg_signal in averaged_signals[label]:
        # חשב כמה חלונות אפשר ליצור מהאות הממוצע
        num_samples = len(avg_signal)
        if num_samples < WINDOW_SIZE_SAMPLES:
            # tqdm.write(f"Warning: Averaged signal for label '{label}' is shorter than window size ({num_samples} < {WINDOW_SIZE_SAMPLES}). Padding or skipping...")
            # אפשרות: רפד באפסים
            padded_signal = np.pad(avg_signal, (0, WINDOW_SIZE_SAMPLES - num_samples), 'constant')
            all_windows.append(padded_signal)
            all_window_labels.append(label)
            # אפשרות: דלג
            # continue
        else:
            # חשב את מספר החלונות המלאים שניתן ליצור
            num_windows = math.floor((num_samples - WINDOW_SIZE_SAMPLES) / HOP_SIZE_SAMPLES) + 1
            for i in range(num_windows):
                start_idx = i * HOP_SIZE_SAMPLES
                end_idx = start_idx + WINDOW_SIZE_SAMPLES
                window = avg_signal[start_idx:end_idx]
                all_windows.append(window)
                all_window_labels.append(label)

print(f"\nFinished windowing. Total windows created: {len(all_windows)}")
if not all_windows: print("Error: No windows created. Check signal lengths and window parameters."); exit()
print("Window examples per label:")
unique_labels, counts = np.unique(all_window_labels, return_counts=True)
for l, c in zip(unique_labels, counts): print(f"- {l}: {c} windows")
print("-" * 30)


# --- 5. יצירת ספקטוגרמות ---
print("\n--- 5. Generating Spectrograms from Windows ---")
all_spectrograms = []
# שומרים את התוויות מהחלונות
all_spec_labels = all_window_labels

for window_signal in tqdm(all_windows, desc="Generating Spectrograms"):
    try:
        frequencies, times, Sxx = sp_signal.spectrogram(
            window_signal, fs=SAMPLING_RATE, window=WINDOW, nperseg=NPERSEG,
            noverlap=NOVERLAP, scaling=SCALING, mode=MODE
        )
        log_Sxx = 10 * np.log10(Sxx + epsilon)
        all_spectrograms.append(log_Sxx)
    except Exception as e:
        print(f"\nError generating spectrogram for a window: {e}")
        # אפשר להוסיף placeholder או פשוט לדלג על החלון הבעייתי
        # all_spectrograms.append(None) # לדוגמה, אם רוצים לשמור על סדר

# הסר דגימות שבהן יצירת הספקטוגרמה נכשלה (אם הוספנו None)
# valid_indices = [i for i, spec in enumerate(all_spectrograms) if spec is not None]
# all_spectrograms = [all_spectrograms[i] for i in valid_indices]
# all_spec_labels = [all_spec_labels[i] for i in valid_indices]


print(f"\nGenerated {len(all_spectrograms)} spectrograms.")
if not all_spectrograms: print("Error: No spectrograms generated."); exit()
print("-" * 30)

# --- 6. ויזואליזציה של דוגמאות ספקטוגרמות ---
print("\n--- 6. Displaying Example Spectrograms ---")
num_examples_to_show = min(9, len(all_spectrograms))
if num_examples_to_show > 0:
    random_indices = np.random.choice(len(all_spectrograms), num_examples_to_show, replace=False)
    plt.figure(figsize=(15, 10))
    plt.suptitle(f'Example Log-Spectrograms (from Averaged & Windowed Signals)', fontsize=16)

    # חישוב צירי זמן ותדר פעם אחת (כי כל החלונות באותו אורך)
    dummy_window = np.zeros(WINDOW_SIZE_SAMPLES)
    frequencies, times, _ = sp_signal.spectrogram(dummy_window, fs=SAMPLING_RATE, nperseg=NPERSEG, noverlap=NOVERLAP)

    for i, idx in enumerate(random_indices):
        spec_to_plot = all_spectrograms[idx]
        label = all_spec_labels[idx]
        ax = plt.subplot(3, 3, i + 1)
        im = ax.pcolormesh(times, frequencies, spec_to_plot, shading='gouraud', cmap='viridis')
        ax.set_title(f"Label: {label} (Example {idx})")
        ax.set_ylabel('Frequency [Hz]')
        ax.set_xlabel('Time [sec]')
        plt.colorbar(im, ax=ax, format='%+2.0f dB')

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
else:
    print("Not enough spectrograms to display examples.")

print("-" * 30)
# --- 7. השלבים הבאים ---
print("\n--- 7. Next Steps ---")
print("Data is now processed:")
print(f"- Averaged signals (from good channels) were created for each event file.")
print(f"- These signals were divided into {len(all_windows)} overlapping windows of {WINDOW_DURATION_SEC} sec.")
print(f"- {len(all_spectrograms)} Log-Spectrograms were generated from these windows.")
print("\nThe variables 'all_spectrograms' (list of 2D numpy arrays) and 'all_spec_labels' (list of strings) contain the data ready for model training.")
print("\nNext steps typically involve:")
print("1. Splitting the data (spectrograms and labels) into Training, Validation, and Test sets.")
print("2. Creating PyTorch/TensorFlow Datasets and DataLoaders.")
print("3. Defining and training your CNN model.")
print("-" * 30)
