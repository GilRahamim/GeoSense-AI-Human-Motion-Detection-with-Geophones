# -*- coding: utf-8 -*-
# --- המשך מהקוד הקודם: יצירת ספקטוגרמות ואימון CNN ---

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import glob
from tqdm import tqdm
from scipy import signal as sp_signal
import itertools
import math
import random
import time

# --- ייבוא ספריות ללמידה עמוקה ---
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms # יכול להיות שימושי לנרמול/אוגמנטציה
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns


# --- 1. תצורה (Configuration) ---
DATA_DIR = 'data'
SAMPLING_RATE = 1000 # Hz
PROBLEM_CHANNELS = {0} # עמודה 0 זוהתה כבעייתית
WINDOW_DURATION_SEC = 5
HOP_RATIO = 0.5
WINDOW_SIZE_SAMPLES = int(WINDOW_DURATION_SEC * SAMPLING_RATE)
HOP_SIZE_SAMPLES = int(WINDOW_SIZE_SAMPLES * HOP_RATIO)
NPERSEG = 256
NOVERLAP_RATIO = 0.75
NOVERLAP = int(NPERSEG * NOVERLAP_RATIO)
WINDOW = 'hann'
SCALING = 'density'
MODE = 'psd'
epsilon = 1e-10

# --- הגדרות אימון ---
SEED = 42 # לתוצאות עקביות
TEST_SIZE = 0.15 # אחוז ל-Test set
VALIDATION_SIZE = 0.15 # אחוז ל-Validation set (מתוך מה שנשאר אחרי Test)
BATCH_SIZE = 32
LEARNING_RATE = 0.0005 # קצב למידה התחלתי (אפשר לשנות)
NUM_EPOCHS = 50      # מספר סיבובי אימון (אפשר להגדיל אם צריך)
PATIENCE = 10        # Early stopping: כמה epochs לחכות ללא שיפור לפני עצירה

print(f"--- Project Configuration ---")
print(f"Train/Val/Test Split: Train=~{1-TEST_SIZE-VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Val=~{VALIDATION_SIZE*(1-TEST_SIZE):.0%}, Test=~{TEST_SIZE:.0%}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"Learning Rate: {LEARNING_RATE}")
print(f"Num Epochs: {NUM_EPOCHS}")
print(f"Early Stopping Patience: {PATIENCE}")
print("-" * 30)

# --- הגדרת Seed ---
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- 2. פונקציות עזר (טעינה, קורלציה, יישור - מהקוד הקודם) ---
# נניח שהפונקציות load_data, calculate_cross_correlation, align_signal קיימות כאן
# ... (העתק את הפונקציות האלה מהקוד הקודם לכאן) ...
def get_label_from_filename(filename): filename_lower = filename.lower(); if 'nothing' in filename_lower: return 'nothing'; elif 'human' in filename_lower or 'man' in filename_lower: return 'human'; elif 'car' in filename_lower: return 'car'; else: return None
def load_data(data_dir): data_structured = {}; labels_by_filename = {}; all_source_keys = []; csv_files = glob.glob(os.path.join(data_dir, '*.csv')); if not csv_files: print("Error: No CSV files found."); exit(); for filepath in tqdm(csv_files, desc="Loading Files", unit="file"): filename = os.path.basename(filepath); label = get_label_from_filename(filename); if label is None: continue; try: if os.path.getsize(filepath) == 0: continue; df = pd.read_csv(filepath, header=None); if df.empty: continue; data_structured[filename] = {}; labels_by_filename[filename] = label; for i, col_name in enumerate(df.columns): if pd.api.types.is_numeric_dtype(df[col_name]): sensor_series = df[col_name].fillna(0).values.astype(np.float64); if len(sensor_series) >= 10 and np.std(sensor_series) > 1e-9: data_structured[filename][i] = sensor_series; all_source_keys.append((filename, i)) except Exception as e: tqdm.write(f"Error processing {filename}: {e}"); print(f"\nFinished loading. Total unique signals loaded: {len(all_source_keys)}"); if not all_source_keys: print("Error: No valid signals loaded."); exit(); print("\nSummary:"); for fname, cols_dict in data_structured.items(): print(f"- {fname} ({labels_by_filename[fname]}): Cols {list(cols_dict.keys())}"); print("-" * 30); return data_structured, labels_by_filename, all_source_keys
def calculate_cross_correlation(sig1, sig2, sampling_rate): sig1 = np.asarray(sig1, dtype=np.float64); sig2 = np.asarray(sig2, dtype=np.float64); sig1 = np.nan_to_num(sig1); sig2 = np.nan_to_num(sig2); if len(sig1) < 2 or len(sig2) < 2: return None, None, None, None, None; mean1, std1 = np.mean(sig1), np.std(sig1); mean2, std2 = np.mean(sig2), np.std(sig2); if std1 < 1e-9 or std2 < 1e-9 : n1, n2 = len(sig1), len(sig2); lags_samples = np.arange(-(n2 - 1), n1); lags_ms = lags_samples * (1000.0 / sampling_rate); return lags_ms, np.zeros(len(lags_ms)), 0.0, 0.0, 0; sig1_norm = (sig1 - mean1) / std1; sig2_norm = (sig2 - mean2) / std2; correlation = np.correlate(sig1_norm, sig2_norm, mode='full'); n1, n2 = len(sig1_norm), len(sig2_norm); lags_samples = np.arange(-(n2 - 1), n1); lags_ms = lags_samples * (1000.0 / sampling_rate); norm_factor = np.sqrt(np.sum(sig1_norm**2) * np.sum(sig2_norm**2)); if norm_factor < 1e-9: normalized_correlation = np.zeros_like(correlation); else: normalized_correlation = correlation / norm_factor; if len(normalized_correlation) == 0: return None, None, None, None, None; peak_index = np.argmax(np.abs(normalized_correlation)); peak_lag_ms = lags_ms[peak_index]; peak_value = normalized_correlation[peak_index]; peak_lag_samples = int(round(peak_lag_ms * sampling_rate / 1000.0)) if peak_lag_ms is not None else 0; if peak_index < 0 or peak_index >= len(lags_ms): peak_lag_ms, peak_value, peak_lag_samples = None, None, 0; return lags_ms, normalized_correlation, peak_lag_ms, peak_value, peak_lag_samples
def align_signal(signal_to_align, n_ref, lag_samples): n_align = len(signal_to_align); aligned_signal = np.zeros(n_ref); if lag_samples > 0: len_to_copy = min(n_ref, n_align - lag_samples); if len_to_copy > 0: aligned_signal[:len_to_copy] = signal_to_align[lag_samples:lag_samples + len_to_copy]; elif lag_samples < 0: start_index_aligned = abs(lag_samples); len_to_copy = min(n_align, n_ref - start_index_aligned); if len_to_copy > 0: aligned_signal[start_index_aligned:start_index_aligned + len_to_copy] = signal_to_align[:len_to_copy]; else: len_to_copy = min(n_ref, n_align); aligned_signal[:len_to_copy] = signal_to_align[:len_to_copy]; return aligned_signal

# --- 3. מיצוע מיושר (כמו בקוד הקודם) ---
print("\n--- 3. Performing Aligned Averaging ---")
data_structured, labels_by_filename, _ = load_data(DATA_DIR)
averaged_signals = {}
processed_files_count = 0
files_to_process = list(data_structured.keys())
for filename in tqdm(files_to_process, desc="Averaging Files"):
    # ... (קוד המיצוע המיושר זהה לקוד הקודם) ...
    if filename not in data_structured: continue
    label = labels_by_filename[filename]; available_cols = sorted(list(data_structured[filename].keys()))
    good_cols_for_file = [col for col in available_cols if col not in PROBLEM_CHANNELS]
    if len(good_cols_for_file) < 1: continue
    elif len(good_cols_for_file) == 1: averaged_signal = data_structured[filename][good_cols_for_file[0]]
    else:
        ref_col_idx = good_cols_for_file[0]; signal_ref = data_structured[filename][ref_col_idx]; n_ref = len(signal_ref)
        sum_aligned_signals = np.copy(signal_ref); num_signals_averaged = 1
        for i in range(1, len(good_cols_for_file)):
            col_to_align_idx = good_cols_for_file[i]; signal_to_align = data_structured[filename][col_to_align_idx]
            _, _, _, _, peak_lag_samples = calculate_cross_correlation(signal_ref, signal_to_align, SAMPLING_RATE)
            if peak_lag_samples is None: continue
            aligned_signal = align_signal(signal_to_align, n_ref, peak_lag_samples)
            sum_aligned_signals += aligned_signal; num_signals_averaged += 1
        if num_signals_averaged > 0: averaged_signal = sum_aligned_signals / num_signals_averaged
        else: averaged_signal = signal_ref
    if label not in averaged_signals: averaged_signals[label] = []
    averaged_signals[label].append(averaged_signal); processed_files_count += 1
print(f"\nFinished averaging. Processed {processed_files_count} files.")

# --- 4. חלוקה לחלונות (כמו בקוד הקודם) ---
print("\n--- 4. Windowing Averaged Signals ---")
all_windows = []
all_window_labels = []
labels_to_process = list(averaged_signals.keys())
for label in tqdm(labels_to_process, desc="Windowing Labels"):
    for avg_signal in averaged_signals[label]:
        num_samples = len(avg_signal)
        if num_samples < WINDOW_SIZE_SAMPLES:
             padded_signal = np.pad(avg_signal, (0, WINDOW_SIZE_SAMPLES - num_samples), 'constant')
             all_windows.append(padded_signal); all_window_labels.append(label)
        else:
            num_windows = math.floor((num_samples - WINDOW_SIZE_SAMPLES) / HOP_SIZE_SAMPLES) + 1
            for i in range(num_windows):
                start_idx = i * HOP_SIZE_SAMPLES; end_idx = start_idx + WINDOW_SIZE_SAMPLES
                window = avg_signal[start_idx:end_idx]; all_windows.append(window); all_window_labels.append(label)
print(f"\nFinished windowing. Total windows created: {len(all_windows)}")
if not all_windows: print("Error: No windows created."); exit()

# --- 5. יצירת ספקטוגרמות (כמו בקוד הקודם) ---
print("\n--- 5. Generating Spectrograms ---")
all_spectrograms = []
all_spec_labels = all_window_labels # Keep labels aligned
for window_signal in tqdm(all_windows, desc="Generating Spectrograms"):
    try:
        frequencies, times, Sxx = sp_signal.spectrogram(window_signal, fs=SAMPLING_RATE, window=WINDOW, nperseg=NPERSEG, noverlap=NOVERLAP, scaling=SCALING, mode=MODE)
        log_Sxx = 10 * np.log10(Sxx + epsilon); all_spectrograms.append(log_Sxx.astype(np.float32)) # Convert to float32 for PyTorch
    except Exception as e: tqdm.write(f"Error generating spectrogram: {e}")
print(f"\nGenerated {len(all_spectrograms)} spectrograms.")
if not all_spectrograms: print("Error: No spectrograms generated."); exit()

# --- 6. הכנת נתונים לאימון (Data Preparation for Training) ---
print("\n--- 6. Preparing Data for Training ---")

# המרת תוויות למספרים
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(all_spec_labels)
num_classes = len(label_encoder.classes_)
print(f"Label encoding: {list(zip(label_encoder.classes_, range(num_classes)))}")
print(f"Number of classes: {num_classes}")

# הוספת מימד "ערוץ" (1) לספקטוגרמות
# Input for Conv2d should be (Batch, Channels, Height, Width)
spectrograms_with_channel = np.array(all_spectrograms)[:, np.newaxis, :, :]
print(f"Spectrogram shape (with channel): {spectrograms_with_channel.shape}") # Should be (num_samples, 1, num_freq_bins, num_time_bins)

# חלוקה ל-Train, Validation, Test
X = spectrograms_with_channel
y = encoded_labels

# First split: Train+Val vs Test
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y # stratify helps keep class proportions
)

# Second split: Train vs Validation
# Adjust validation size based on the remaining data
val_size_adjusted = VALIDATION_SIZE / (1 - TEST_SIZE)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=val_size_adjusted, random_state=SEED, stratify=y_train_val
)

print(f"\nData split:")
print(f"- Training set: {len(X_train)} samples")
print(f"- Validation set: {len(X_val)} samples")
print(f"- Test set: {len(X_test)} samples")

# יצירת PyTorch Datasets
class SpectrogramDataset(Dataset):
    def __init__(self, features, labels, transform=None):
        self.features = torch.from_numpy(features).float()
        self.labels = torch.from_numpy(labels).long()
        self.transform = transform # For potential augmentations/normalization

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        sample = self.features[idx]
        label = self.labels[idx]
        if self.transform:
            sample = self.transform(sample)
        return sample, label

# --- ניתן להוסיף כאן נרמול או אוגמנטציה אם רוצים ---
# transform = transforms.Compose([
#     # transforms.Normalize(mean=[...], std=[...]) # Calculate mean/std on training set
# ])
train_dataset = SpectrogramDataset(X_train, y_train)
val_dataset = SpectrogramDataset(X_val, y_val)
test_dataset = SpectrogramDataset(X_test, y_test)

# יצירת DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

print("\nDataLoaders created.")
print("-" * 30)


# --- 7. הגדרת מודל CNN (Simple Example) ---
print("\n--- 7. Defining CNN Model ---")

class SimpleCNN(nn.Module):
    def __init__(self, num_classes, input_height, input_width):
        super(SimpleCNN, self).__init__()
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1), # Input channel = 1
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2) # Halves height and width
        )
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        # חישוב גודל הקלט לשכבה הלינארית אחרי שכבות ה-Conv/Pool
        # הגובה והרוחב מצטמצמים פי 2 שלוש פעמים (2*2*2 = 8)
        flattened_height = input_height // 8
        flattened_width = input_width // 8
        self.flattened_size = 64 * flattened_height * flattened_width

        self.fc_block = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.flattened_size, 128),
            nn.ReLU(),
            nn.Dropout(0.5), # Dropout for regularization
            nn.Linear(128, num_classes) # Output layer
        )

    def forward(self, x):
        # print("Input shape:", x.shape) # Debug shape
        x = self.conv_block1(x)
        # print("After conv1 shape:", x.shape)
        x = self.conv_block2(x)
        # print("After conv2 shape:", x.shape)
        x = self.conv_block3(x)
        # print("After conv3 shape:", x.shape)
        if x.shape[1] * x.shape[2] * x.shape[3] != self.flattened_size:
             print(f"Warning: Actual flattened size {x.shape[1]*x.shape[2]*x.shape[3]} != calculated {self.flattened_size}")
             # Fallback - adaptive pooling or recalculate
             # For now, let's see if it works as calculated
             # Or use AdaptiveAvgPool2d
             # pool = nn.AdaptiveAvgPool2d((1,1))
             # x = pool(x)
             # self.flattened_size = 64 # Adjust if using adaptive pool
             pass # Let the flatten/linear layer handle it or raise error

        x = self.fc_block(x)
        # print("Output shape:", x.shape)
        return x

# יצירת המודל
# קבלת גודל הספקטוגרמה מהדגימה הראשונה
spec_height = spectrograms_with_channel.shape[2]
spec_width = spectrograms_with_channel.shape[3]
model = SimpleCNN(num_classes=num_classes, input_height=spec_height, input_width=spec_width)

# העברת המודל ל-GPU אם זמין
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"Model defined and moved to {device}.")
# הדפסת המבנה (אופציונלי)
# print(model)
print("-" * 30)


# --- 8. הגדרת אימון (Loss, Optimizer) ---
print("\n--- 8. Setting up Training ---")
criterion = nn.CrossEntropyLoss() # מתאים לסיווג רב-קלאסי
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
# אפשר להוסיף Scheduler להורדת קצב למידה (ראה גרף LR מקורי)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1) # Example: reduce LR every 20 epochs

print("Loss function: CrossEntropyLoss")
print(f"Optimizer: Adam (LR={LEARNING_RATE})")
print(f"Scheduler: StepLR (step=20, gamma=0.1)")
print("-" * 30)

# --- 9. לולאת אימון ואימות (Training and Validation Loop) ---
print("\n--- 9. Starting Training ---")
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []
best_val_accuracy = 0.0
epochs_no_improve = 0
training_start_time = time.time()

for epoch in range(NUM_EPOCHS):
    epoch_start_time = time.time()
    # --- שלב האימון ---
    model.train() # מעבר למצב אימון
    running_loss = 0.0
    correct_train = 0
    total_train = 0

    train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Train]", leave=False)
    for inputs, labels in train_pbar:
        inputs, labels = inputs.to(device), labels.to(device)

        # איפוס גרדיאנטים
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # סטטיסטיקות
        running_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()
        train_pbar.set_postfix({'Loss': loss.item()}) # Show loss in progress bar

    epoch_train_loss = running_loss / len(train_loader.dataset)
    epoch_train_acc = 100.0 * correct_train / total_train
    train_losses.append(epoch_train_loss)
    train_accuracies.append(epoch_train_acc)

    # --- שלב האימות ---
    model.eval() # מעבר למצב הערכה (מכבה Dropout, BatchNorm מתנהג אחרת)
    running_val_loss = 0.0
    correct_val = 0
    total_val = 0

    val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS} [Val]", leave=False)
    with torch.no_grad(): # לא מחשבים גרדיאנטים באימות
        for inputs, labels in val_pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            running_val_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()
            val_pbar.set_postfix({'Loss': loss.item()})

    epoch_val_loss = running_val_loss / len(val_loader.dataset)
    epoch_val_acc = 100.0 * correct_val / total_val
    val_losses.append(epoch_val_loss)
    val_accuracies.append(epoch_val_acc)

    # עדכון קצב למידה (אם יש Scheduler)
    current_lr = optimizer.param_groups[0]['lr']
    scheduler.step()

    epoch_duration = time.time() - epoch_start_time
    print(f"Epoch {epoch+1}/{NUM_EPOCHS} [{epoch_duration:.1f}s] - "
          f"LR: {current_lr:.1e} - "
          f"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% - "
          f"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%")

    # --- בדיקת שיפור ושמירת המודל הטוב ביותר ---
    if epoch_val_acc > best_val_accuracy:
        print(f"  Validation accuracy improved ({best_val_accuracy:.2f}% -> {epoch_val_acc:.2f}%). Saving model...")
        best_val_accuracy = epoch_val_acc
        # שמירת המודל
        model_save_path = "best_cnn_model.pth"
        torch.save(model.state_dict(), model_save_path)
        epochs_no_improve = 0 # אפס מונה המתנה
    else:
        epochs_no_improve += 1
        print(f"  Validation accuracy did not improve for {epochs_no_improve} epoch(s). Best: {best_val_accuracy:.2f}%")

    # --- Early Stopping ---
    if epochs_no_improve >= PATIENCE:
        print(f"\nEarly stopping triggered after {epoch+1} epochs.")
        break

training_duration = time.time() - training_start_time
print(f"\n--- Training Finished ({training_duration / 60:.1f} minutes) ---")
print(f"Best Validation Accuracy achieved: {best_val_accuracy:.2f}%")
print("-" * 30)

# --- 10. הערכה סופית על קבוצת המבחן (Test Set Evaluation) ---
print("\n--- 10. Evaluating on Test Set ---")

# טען את המודל הטוב ביותר שנשמר
print(f"Loading best model from {model_save_path}...")
model.load_state_dict(torch.load(model_save_path))
model.to(device) # ודא שהמודל על המכשיר הנכון
model.eval() # מצב הערכה

all_preds = []
all_labels = []

with torch.no_grad():
    for inputs, labels in tqdm(test_loader, desc="Testing"):
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)

        all_preds.extend(predicted.cpu().numpy()) # העבר ל-CPU והמר ל-numpy
        all_labels.extend(labels.cpu().numpy())

# חישוב מדדי ביצועים
test_accuracy = accuracy_score(all_labels, all_preds)
print(f"\nTest Accuracy: {test_accuracy * 100.0:.2f}%")

print("\nClassification Report:")
# קבלת שמות התוויות מה-encoder
class_names = label_encoder.classes_
print(classification_report(all_labels, all_preds, target_names=class_names))

print("\nConfusion Matrix:")
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Test Set')
plt.show()
print("-" * 30)

# --- 11. הצגת גרפי אימון (Training Curves) ---
print("\n--- 11. Plotting Training Curves ---")

epochs_range = range(1, len(train_losses) + 1)

plt.figure(figsize=(12, 5))

# גרף Loss
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_losses, label='Training Loss')
plt.plot(epochs_range, val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

# גרף Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_accuracies, label='Training Accuracy')
plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print("\n--- Full Process Complete ---")
